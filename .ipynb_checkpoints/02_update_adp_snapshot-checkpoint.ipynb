{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3bece0-a796-4816-9447-e3101f8e99ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [2023] DISCOVERY STEP 0 | users=5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] leagues chunk 1 (5): 100%|████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 31.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Leagues fetched=172 | new=172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] league users chunk 1 (172): 100%|█████████████████████████████████████████████| 172/172 [00:15<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Next frontier users=1280 | total users seen=1285\n",
      "\n",
      "=== [2023] DISCOVERY STEP 1 | users=1280 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] leagues chunk 1 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 325.44it/s]\n",
      "[2023] leagues chunk 2 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 403.41it/s]\n",
      "[2023] leagues chunk 3 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 341.59it/s]\n",
      "[2023] leagues chunk 4 (80): 100%|████████████████████████████████████████████████████| 80/80 [00:00<00:00, 512.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Leagues fetched=17407 | new=17237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] league users chunk 1 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 485.94it/s]\n",
      "[2023] league users chunk 2 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 439.40it/s]\n",
      "[2023] league users chunk 3 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 452.03it/s]\n",
      "[2023] league users chunk 4 (400): 100%|████████████████████████████████████████████| 400/400 [00:01<00:00, 392.46it/s]\n",
      "[2023] league users chunk 5 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 427.35it/s]\n",
      "[2023] league users chunk 6 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 464.36it/s]\n",
      "[2023] league users chunk 7 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 422.08it/s]\n",
      "[2023] league users chunk 8 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 404.52it/s]\n",
      "[2023] league users chunk 9 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 489.12it/s]\n",
      "[2023] league users chunk 10 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 462.88it/s]\n",
      "[2023] league users chunk 11 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 250.23it/s]\n",
      "[2023] league users chunk 12 (400): 100%|████████████████████████████████████████████| 400/400 [00:27<00:00, 14.31it/s]\n",
      "[2023] league users chunk 13 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 434.24it/s]\n",
      "[2023] league users chunk 14 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 439.71it/s]\n",
      "[2023] league users chunk 15 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 393.43it/s]\n",
      "[2023] league users chunk 16 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 491.21it/s]\n",
      "[2023] league users chunk 17 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 386.69it/s]\n",
      "[2023] league users chunk 18 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 451.40it/s]\n",
      "[2023] league users chunk 19 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 431.77it/s]\n",
      "[2023] league users chunk 20 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 436.89it/s]\n",
      "[2023] league users chunk 21 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 466.82it/s]\n",
      "[2023] league users chunk 22 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 479.48it/s]\n",
      "[2023] league users chunk 23 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 261.19it/s]\n",
      "[2023] league users chunk 24 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 454.71it/s]\n",
      "[2023] league users chunk 25 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 440.49it/s]\n",
      "[2023] league users chunk 26 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 478.13it/s]\n",
      "[2023] league users chunk 27 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 469.75it/s]\n",
      "[2023] league users chunk 28 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 386.61it/s]\n",
      "[2023] league users chunk 29 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 443.31it/s]\n",
      "[2023] league users chunk 30 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 429.98it/s]\n",
      "[2023] league users chunk 31 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 485.14it/s]\n",
      "[2023] league users chunk 32 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 470.14it/s]\n",
      "[2023] league users chunk 33 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 443.81it/s]\n",
      "[2023] league users chunk 34 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 439.45it/s]\n",
      "[2023] league users chunk 35 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 439.46it/s]\n",
      "[2023] league users chunk 36 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 226.38it/s]\n",
      "[2023] league users chunk 37 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 448.30it/s]\n",
      "[2023] league users chunk 38 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 463.61it/s]\n",
      "[2023] league users chunk 39 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 457.75it/s]\n",
      "[2023] league users chunk 40 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 482.73it/s]\n",
      "[2023] league users chunk 41 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 464.42it/s]\n",
      "[2023] league users chunk 42 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 434.37it/s]\n",
      "[2023] league users chunk 43 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 407.41it/s]\n",
      "[2023] league users chunk 44 (37): 100%|██████████████████████████████████████████████| 37/37 [00:00<00:00, 294.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Next frontier users=61270 | total users seen=62555\n",
      "\n",
      "=== [2023] DISCOVERY STEP 2 | users=2500 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] leagues chunk 1 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 235.86it/s]\n",
      "[2023] leagues chunk 2 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:03<00:00, 123.55it/s]\n",
      "[2023] leagues chunk 3 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 334.75it/s]\n",
      "[2023] leagues chunk 4 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:03<00:00, 130.97it/s]\n",
      "[2023] leagues chunk 5 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 243.97it/s]\n",
      "[2023] leagues chunk 6 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 211.73it/s]\n",
      "[2023] leagues chunk 7 (100): 100%|█████████████████████████████████████████████████| 100/100 [00:00<00:00, 333.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Leagues fetched=39224 | new=29372\n",
      "[2023] Hit MAX_LEAGUES_TOTAL cap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] drafts chunk 1 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 490.20it/s]\n",
      "[2023] drafts chunk 2 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 496.46it/s]\n",
      "[2023] drafts chunk 3 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 491.12it/s]\n",
      "[2023] drafts chunk 4 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 481.29it/s]\n",
      "[2023] drafts chunk 5 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 497.93it/s]\n",
      "[2023] drafts chunk 6 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 512.17it/s]\n",
      "[2023] drafts chunk 7 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 498.81it/s]\n",
      "[2023] drafts chunk 8 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 504.37it/s]\n",
      "[2023] drafts chunk 9 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 414.44it/s]\n",
      "[2023] drafts chunk 10 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 494.47it/s]\n",
      "[2023] drafts chunk 11 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 364.79it/s]\n",
      "[2023] drafts chunk 12 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 510.28it/s]\n",
      "[2023] drafts chunk 13 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 511.62it/s]\n",
      "[2023] drafts chunk 14 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 411.06it/s]\n",
      "[2023] drafts chunk 15 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 496.92it/s]\n",
      "[2023] drafts chunk 16 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 484.77it/s]\n",
      "[2023] drafts chunk 17 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 487.92it/s]\n",
      "[2023] drafts chunk 18 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 443.33it/s]\n",
      "[2023] drafts chunk 19 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 506.84it/s]\n",
      "[2023] drafts chunk 20 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 489.63it/s]\n",
      "[2023] drafts chunk 21 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 500.54it/s]\n",
      "[2023] drafts chunk 22 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 523.54it/s]\n",
      "[2023] drafts chunk 23 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 461.95it/s]\n",
      "[2023] drafts chunk 24 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 479.81it/s]\n",
      "[2023] drafts chunk 25 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 447.50it/s]\n",
      "[2023] drafts chunk 26 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 479.46it/s]\n",
      "[2023] drafts chunk 27 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 495.95it/s]\n",
      "[2023] drafts chunk 28 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 491.71it/s]\n",
      "[2023] drafts chunk 29 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 502.99it/s]\n",
      "[2023] drafts chunk 30 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 473.41it/s]\n",
      "[2023] drafts chunk 31 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 497.00it/s]\n",
      "[2023] drafts chunk 32 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 510.73it/s]\n",
      "[2023] drafts chunk 33 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:40<00:00,  9.78it/s]\n",
      "[2023] drafts chunk 34 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 267.06it/s]\n",
      "[2023] drafts chunk 35 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 456.27it/s]\n",
      "[2023] drafts chunk 36 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 484.83it/s]\n",
      "[2023] drafts chunk 37 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 436.81it/s]\n",
      "[2023] drafts chunk 38 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 460.39it/s]\n",
      "[2023] drafts chunk 39 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 427.55it/s]\n",
      "[2023] drafts chunk 40 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 495.51it/s]\n",
      "[2023] drafts chunk 41 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 490.86it/s]\n",
      "[2023] drafts chunk 42 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 424.08it/s]\n",
      "[2023] drafts chunk 43 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 487.15it/s]\n",
      "[2023] drafts chunk 44 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 484.17it/s]\n",
      "[2023] drafts chunk 45 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 394.13it/s]\n",
      "[2023] drafts chunk 46 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 437.78it/s]\n",
      "[2023] drafts chunk 47 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 349.37it/s]\n",
      "[2023] drafts chunk 48 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 444.96it/s]\n",
      "[2023] drafts chunk 49 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 472.07it/s]\n",
      "[2023] drafts chunk 50 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 464.72it/s]\n",
      "[2023] drafts chunk 51 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 474.71it/s]\n",
      "[2023] drafts chunk 52 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 469.49it/s]\n",
      "[2023] drafts chunk 53 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 454.77it/s]\n",
      "[2023] drafts chunk 54 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 407.63it/s]\n",
      "[2023] drafts chunk 55 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 461.41it/s]\n",
      "[2023] drafts chunk 56 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 510.51it/s]\n",
      "[2023] drafts chunk 57 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 449.28it/s]\n",
      "[2023] drafts chunk 58 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 494.27it/s]\n",
      "[2023] drafts chunk 59 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 460.98it/s]\n",
      "[2023] drafts chunk 60 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 456.21it/s]\n",
      "[2023] drafts chunk 61 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 489.66it/s]\n",
      "[2023] drafts chunk 62 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 482.96it/s]\n",
      "[2023] drafts chunk 63 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 440.20it/s]\n",
      "[2023] drafts chunk 64 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 402.69it/s]\n",
      "[2023] drafts chunk 65 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 477.93it/s]\n",
      "[2023] drafts chunk 66 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 462.84it/s]\n",
      "[2023] drafts chunk 67 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 441.69it/s]\n",
      "[2023] drafts chunk 68 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 484.81it/s]\n",
      "[2023] drafts chunk 69 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 500.39it/s]\n",
      "[2023] drafts chunk 70 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 469.87it/s]\n",
      "[2023] drafts chunk 71 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 485.64it/s]\n",
      "[2023] drafts chunk 72 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 465.13it/s]\n",
      "[2023] drafts chunk 73 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 466.62it/s]\n",
      "[2023] drafts chunk 74 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 489.25it/s]\n",
      "[2023] drafts chunk 75 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 433.75it/s]\n",
      "[2023] drafts chunk 76 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 475.84it/s]\n",
      "[2023] drafts chunk 77 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 480.32it/s]\n",
      "[2023] drafts chunk 78 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 490.88it/s]\n",
      "[2023] drafts chunk 79 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 313.63it/s]\n",
      "[2023] drafts chunk 80 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 495.99it/s]\n",
      "[2023] drafts chunk 81 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 489.84it/s]\n",
      "[2023] drafts chunk 82 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 469.71it/s]\n",
      "[2023] drafts chunk 83 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 465.09it/s]\n",
      "[2023] drafts chunk 84 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 487.12it/s]\n",
      "[2023] drafts chunk 85 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 442.71it/s]\n",
      "[2023] drafts chunk 86 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 479.21it/s]\n",
      "[2023] drafts chunk 87 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 408.52it/s]\n",
      "[2023] drafts chunk 88 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 467.34it/s]\n",
      "[2023] drafts chunk 89 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 508.26it/s]\n",
      "[2023] drafts chunk 90 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 453.80it/s]\n",
      "[2023] drafts chunk 91 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 497.85it/s]\n",
      "[2023] drafts chunk 92 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 463.58it/s]\n",
      "[2023] drafts chunk 93 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 489.51it/s]\n",
      "[2023] drafts chunk 94 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 499.98it/s]\n",
      "[2023] drafts chunk 95 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 515.29it/s]\n",
      "[2023] drafts chunk 96 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 481.79it/s]\n",
      "[2023] drafts chunk 97 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 481.93it/s]\n",
      "[2023] drafts chunk 98 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 508.32it/s]\n",
      "[2023] drafts chunk 99 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 291.28it/s]\n",
      "[2023] drafts chunk 100 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 492.03it/s]\n",
      "[2023] drafts chunk 101 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 494.22it/s]\n",
      "[2023] drafts chunk 102 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 486.65it/s]\n",
      "[2023] drafts chunk 103 (400): 100%|████████████████████████████████████████████████| 400/400 [00:01<00:00, 300.92it/s]\n",
      "[2023] drafts chunk 104 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 463.56it/s]\n",
      "[2023] drafts chunk 105 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 506.37it/s]\n",
      "[2023] drafts chunk 106 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 405.31it/s]\n",
      "[2023] drafts chunk 107 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 481.36it/s]\n",
      "[2023] drafts chunk 108 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 502.05it/s]\n",
      "[2023] drafts chunk 109 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 471.51it/s]\n",
      "[2023] drafts chunk 110 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 457.08it/s]\n",
      "[2023] drafts chunk 111 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 412.95it/s]\n",
      "[2023] drafts chunk 112 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 428.59it/s]\n",
      "[2023] drafts chunk 113 (400): 100%|████████████████████████████████████████████████| 400/400 [00:01<00:00, 312.02it/s]\n",
      "[2023] drafts chunk 114 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 492.46it/s]\n",
      "[2023] drafts chunk 115 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 473.75it/s]\n",
      "[2023] drafts chunk 116 (400): 100%|████████████████████████████████████████████████| 400/400 [00:00<00:00, 472.27it/s]\n",
      "[2023] drafts chunk 117 (381): 100%|████████████████████████████████████████████████| 381/381 [00:00<00:00, 480.31it/s]\n",
      "C:\\Users\\lgilb\\AppData\\Local\\Temp\\ipykernel_24792\\3996958976.py:216: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
      "[2023] picks chunk 1 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:02<00:00, 164.69it/s]\n",
      "[2023] picks chunk 2 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:02<00:00, 176.19it/s]\n",
      "[2023] picks chunk 3 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 209.98it/s]\n",
      "[2023] picks chunk 4 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 271.34it/s]\n",
      "[2023] picks chunk 5 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:03<00:00, 120.48it/s]\n",
      "[2023] picks chunk 6 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 260.94it/s]\n",
      "[2023] picks chunk 7 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:02<00:00, 197.97it/s]\n",
      "[2023] picks chunk 8 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 311.84it/s]\n",
      "[2023] picks chunk 9 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 218.68it/s]\n",
      "[2023] picks chunk 10 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 276.62it/s]\n",
      "[2023] picks chunk 11 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:04<00:00, 93.70it/s]\n",
      "[2023] picks chunk 12 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:02<00:00, 152.08it/s]\n",
      "[2023] picks chunk 13 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:02<00:00, 186.53it/s]\n",
      "[2023] picks chunk 14 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:02<00:00, 164.43it/s]\n",
      "[2023] picks chunk 15 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:03<00:00, 121.85it/s]\n",
      "[2023] picks chunk 16 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:08<00:00, 44.59it/s]\n",
      "[2023] picks chunk 17 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:02<00:00, 144.90it/s]\n",
      "[2023] picks chunk 18 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:03<00:00, 113.70it/s]\n",
      "[2023] picks chunk 19 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 228.07it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.38 MiB for an array with shape (60275, 3) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 344\u001b[0m\n\u001b[0;32m    340\u001b[0m drafts_df\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RAW_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrafts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrafts_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    342\u001b[0m draft_catalog \u001b[38;5;241m=\u001b[39m build_draft_catalog(drafts_df)\n\u001b[1;32m--> 344\u001b[0m picks_df \u001b[38;5;241m=\u001b[39m fetch_picks_for_completed_drafts(drafts_df, season)\n\u001b[0;32m    345\u001b[0m picks_df\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RAW_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpicks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpicks_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    347\u001b[0m adp_ts \u001b[38;5;241m=\u001b[39m compute_adp_time_series(picks_df, draft_catalog)\n",
      "Cell \u001b[1;32mIn[17], line 241\u001b[0m, in \u001b[0;36mfetch_picks_for_completed_drafts\u001b[1;34m(drafts_df, season)\u001b[0m\n\u001b[0;32m    239\u001b[0m         buf\u001b[38;5;241m.\u001b[39mappend(pick_to_row(p, draft_id))\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf:\n\u001b[1;32m--> 241\u001b[0m     parts\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame(buf))\n\u001b[0;32m    242\u001b[0m     buf \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(urls) \u001b[38;5;241m>\u001b[39m CHUNK_SIZE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    854\u001b[0m         data,\n\u001b[0;32m    855\u001b[0m         columns,\n\u001b[0;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    857\u001b[0m         dtype,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:837\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m--> 837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m    839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:924\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# classes\u001b[39;00m\n\u001b[0;32m    922\u001b[0m data \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m content \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mdicts_to_array(data, \u001b[38;5;28mlist\u001b[39m(columns))\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mlib.pyx:395\u001b[0m, in \u001b[0;36mpandas._libs.lib.dicts_to_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.38 MiB for an array with shape (60275, 3) and data type object"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 02_update_adp_snapshot.ipynb\n",
    "# Refresh current-season ADP time series (time = draft timestamps)\n",
    "# ============================================================\n",
    "import os\n",
    "import time\n",
    "from typing import Any, List, Tuple, Optional, Set\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "CURRENT_SEASON = 2023\n",
    "\n",
    "SEED_USERS = [\n",
    "    (\"camsnotsober\", \"567994319854673920\"),\n",
    "    (\"dynastybuck\", \"332066581859282944\"),\n",
    "    (\"curtistodd\", \"568256222760906752\"),\n",
    "    (\"elnostrathomas\", \"387839476958965760\"),\n",
    "    (\"coombesie9\", \"386648007942254592\"),\n",
    "]\n",
    "\n",
    "MAX_EXPANSION_STEPS = 2\n",
    "MAX_USERS_PER_STEP = 2500\n",
    "MAX_LEAGUES_TOTAL = 20000\n",
    "\n",
    "MAX_WORKERS = 40\n",
    "CHUNK_SIZE = 400\n",
    "SLEEP_BETWEEN_CHUNKS_SEC = 8\n",
    "\n",
    "ROOT_DIR = \"sleeper_dynasty_adp\"\n",
    "RAW_DIR  = os.path.join(ROOT_DIR, \"data\", \"raw\")\n",
    "SNAP_DIR = os.path.join(ROOT_DIR, \"data\", \"snapshots\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(SNAP_DIR, exist_ok=True)\n",
    "\n",
    "for sub in [\"leagues\", \"league_users\", \"drafts\", \"picks\", \"players\"]:\n",
    "    os.makedirs(os.path.join(RAW_DIR, sub), exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# HTTP\n",
    "# ----------------------------\n",
    "BASE = \"https://api.sleeper.app/v1\"\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Sleeper-Dynasty-ADP/1.0\"})\n",
    "\n",
    "def get_json(url: str, timeout: int = 30, retries: int = 4, backoff: float = 1.8) -> Any:\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=timeout)\n",
    "            if r.status_code == 429:\n",
    "                time.sleep(min(30, (backoff ** i) + 1))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(min(30, (backoff ** i) + 0.5))\n",
    "    raise RuntimeError(f\"GET failed: {url}\\nLast error: {last_err}\")\n",
    "\n",
    "def chunked(lst: List[Any], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def parallel_fetch(urls: List[str], desc: str) -> List[Tuple[str, Any, Optional[str]]]:\n",
    "    out = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(get_json, u): u for u in urls}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=desc):\n",
    "            u = futs[fut]\n",
    "            try:\n",
    "                out.append((u, fut.result(), None))\n",
    "            except Exception as e:\n",
    "                out.append((u, None, str(e)))\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# URL helpers\n",
    "# ----------------------------\n",
    "def url_user_leagues(user_id: str, season: int) -> str:\n",
    "    return f\"{BASE}/user/{user_id}/leagues/nfl/{season}\"\n",
    "\n",
    "def url_league_users(league_id: str) -> str:\n",
    "    return f\"{BASE}/league/{league_id}/users\"\n",
    "\n",
    "def url_league_drafts(league_id: str) -> str:\n",
    "    return f\"{BASE}/league/{league_id}/drafts\"\n",
    "\n",
    "def url_draft_picks(draft_id: str) -> str:\n",
    "    return f\"{BASE}/draft/{draft_id}/picks\"\n",
    "\n",
    "# ----------------------------\n",
    "# Discovery\n",
    "# ----------------------------\n",
    "def fetch_leagues_for_users(user_ids: List[str], season: int) -> pd.DataFrame:\n",
    "    urls = [url_user_leagues(uid, season) for uid in user_ids]\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season}] leagues chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            for lg in data:\n",
    "                lg[\"_season\"] = season\n",
    "                rows.append(lg)\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.json_normalize(rows).drop_duplicates(subset=[\"league_id\"])\n",
    "\n",
    "def fetch_users_for_leagues(league_ids: List[str], season: int) -> pd.DataFrame:\n",
    "    urls = [url_league_users(lid) for lid in league_ids]\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season}] league users chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            league_id = u.split(\"/league/\")[1].split(\"/users\")[0]\n",
    "            for usr in data:\n",
    "                usr[\"_league_id\"] = league_id\n",
    "                rows.append(usr)\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "def discover_leagues(season: int, seed_users: List[Tuple[str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    frontier_users = [uid for _name, uid in seed_users]\n",
    "    seen_users: Set[str] = set(frontier_users)\n",
    "    seen_leagues: Set[str] = set()\n",
    "    leagues_parts, memberships_parts = [], []\n",
    "\n",
    "    for step in range(MAX_EXPANSION_STEPS + 1):\n",
    "        frontier_users = frontier_users[:MAX_USERS_PER_STEP]\n",
    "        print(f\"\\n=== [{season}] DISCOVERY STEP {step} | users={len(frontier_users)} ===\")\n",
    "\n",
    "        leagues_df = fetch_leagues_for_users(frontier_users, season)\n",
    "        if leagues_df.empty:\n",
    "            break\n",
    "\n",
    "        new_leagues_df = leagues_df[~leagues_df[\"league_id\"].astype(str).isin(seen_leagues)].copy()\n",
    "        print(f\"[{season}] Leagues fetched={len(leagues_df)} | new={len(new_leagues_df)}\")\n",
    "        if new_leagues_df.empty:\n",
    "            break\n",
    "\n",
    "        leagues_parts.append(new_leagues_df)\n",
    "        new_league_ids = new_leagues_df[\"league_id\"].astype(str).tolist()\n",
    "        seen_leagues.update(new_league_ids)\n",
    "\n",
    "        if len(seen_leagues) >= MAX_LEAGUES_TOTAL:\n",
    "            print(f\"[{season}] Hit MAX_LEAGUES_TOTAL cap.\")\n",
    "            break\n",
    "\n",
    "        mem_df = fetch_users_for_leagues(new_league_ids, season)\n",
    "        if not mem_df.empty:\n",
    "            memberships_parts.append(mem_df)\n",
    "\n",
    "        if step == MAX_EXPANSION_STEPS or mem_df.empty or \"user_id\" not in mem_df.columns:\n",
    "            break\n",
    "\n",
    "        discovered_users = mem_df[\"user_id\"].dropna().astype(str).unique().tolist()\n",
    "        frontier_users = [u for u in discovered_users if u not in seen_users]\n",
    "        seen_users.update(frontier_users)\n",
    "        print(f\"[{season}] Next frontier users={len(frontier_users)} | total users seen={len(seen_users)}\")\n",
    "\n",
    "    leagues_out = pd.concat(leagues_parts, ignore_index=True) if leagues_parts else pd.DataFrame()\n",
    "    memberships_out = pd.concat(memberships_parts, ignore_index=True) if memberships_parts else pd.DataFrame()\n",
    "    return leagues_out, memberships_out\n",
    "\n",
    "# ----------------------------\n",
    "# Drafts + Picks\n",
    "# ----------------------------\n",
    "def draft_to_row(d: dict, league_id: str, season: int) -> dict:\n",
    "    md = d.get(\"metadata\") or {}\n",
    "    st = d.get(\"settings\") or {}\n",
    "    return {\n",
    "        \"draft_id\": str(d.get(\"draft_id\") or \"\"),\n",
    "        \"league_id\": str(league_id),\n",
    "        \"season\": int(season),\n",
    "        \"status\": d.get(\"status\"),\n",
    "        \"type\": d.get(\"type\"),\n",
    "        \"start_time\": d.get(\"start_time\"),\n",
    "        \"md_scoring_type\": md.get(\"scoring_type\"),\n",
    "        \"st_teams\": st.get(\"teams\"),\n",
    "        \"st_rounds\": st.get(\"rounds\"),\n",
    "        \"st_slots_super_flex\": st.get(\"slots_super_flex\"),\n",
    "    }\n",
    "\n",
    "def fetch_drafts_for_leagues(league_ids: List[str], season: int) -> pd.DataFrame:\n",
    "    urls = [url_league_drafts(lid) for lid in league_ids]\n",
    "    parts, buf = [], []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season}] drafts chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            league_id = u.split(\"/league/\")[1].split(\"/drafts\")[0]\n",
    "            for d in data:\n",
    "                row = draft_to_row(d, league_id, season)\n",
    "                if row[\"draft_id\"]:\n",
    "                    buf.append(row)\n",
    "        if buf:\n",
    "            parts.append(pd.DataFrame(buf).drop_duplicates(subset=[\"draft_id\"]))\n",
    "            buf = []\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "    return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "def pick_to_row(p: dict, draft_id: str) -> dict:\n",
    "    return {\n",
    "        \"draft_id\": str(draft_id),\n",
    "        \"player_id\": str(p.get(\"player_id\")) if p.get(\"player_id\") is not None else None,\n",
    "        \"pick_no\": p.get(\"pick_no\"),\n",
    "    }\n",
    "\n",
    "def fetch_picks_for_completed_drafts(drafts_df: pd.DataFrame, season: int) -> pd.DataFrame:\n",
    "    completed_ids = (\n",
    "        drafts_df.loc[drafts_df[\"status\"].astype(str).str.lower() == \"complete\", \"draft_id\"]\n",
    "        .astype(str).unique().tolist()\n",
    "    )\n",
    "    urls = [url_draft_picks(did) for did in completed_ids]\n",
    "    parts, buf = [], []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season}] picks chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            draft_id = u.split(\"/draft/\")[1].split(\"/picks\")[0]\n",
    "            for p in data:\n",
    "                buf.append(pick_to_row(p, draft_id))\n",
    "        if buf:\n",
    "            parts.append(pd.DataFrame(buf))\n",
    "            buf = []\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "    return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "def build_draft_catalog(drafts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = drafts_df.copy()\n",
    "    for c in [\"draft_id\", \"league_id\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "\n",
    "    for c in [\"start_time\", \"st_teams\", \"st_rounds\", \"st_slots_super_flex\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df[\"start_dt\"] = pd.to_datetime(df[\"start_time\"], unit=\"ms\", utc=True, errors=\"coerce\")\n",
    "    df[\"start_month\"] = df[\"start_dt\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "    df[\"is_dynasty\"] = df[\"md_scoring_type\"].astype(str).str.startswith(\"dynasty_\", na=False)\n",
    "    df[\"is_superflex\"] = df[\"st_slots_super_flex\"].fillna(0) >= 1\n",
    "\n",
    "    def dynasty_class(r):\n",
    "        if not r[\"is_dynasty\"]:\n",
    "            return \"non_dynasty\"\n",
    "        if pd.notna(r[\"st_rounds\"]) and r[\"st_rounds\"] <= 6:\n",
    "            return \"rookie\"\n",
    "        if pd.notna(r[\"st_rounds\"]) and r[\"st_rounds\"] >= 14:\n",
    "            return \"startup\"\n",
    "        return \"other\"\n",
    "\n",
    "    df[\"dynasty_class\"] = df.apply(dynasty_class, axis=1)\n",
    "    return df\n",
    "\n",
    "def compute_adp_time_series(picks_df: pd.DataFrame, draft_catalog: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = picks_df.copy()\n",
    "    p[\"pick_no\"] = pd.to_numeric(p[\"pick_no\"], errors=\"coerce\")\n",
    "    p[\"draft_id\"] = p[\"draft_id\"].astype(str)\n",
    "    p[\"player_id\"] = p[\"player_id\"].astype(str)\n",
    "\n",
    "    d = draft_catalog[[\n",
    "        \"draft_id\", \"season\", \"type\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\",\n",
    "        \"dynasty_class\", \"start_month\"\n",
    "    ]].copy()\n",
    "    d[\"draft_id\"] = d[\"draft_id\"].astype(str)\n",
    "\n",
    "    m = p.merge(d, on=\"draft_id\", how=\"left\")\n",
    "    m = m[m[\"pick_no\"].notna() & m[\"player_id\"].notna()].copy()\n",
    "    m = m[m[\"dynasty_class\"].isin([\"startup\", \"rookie\"])].copy()\n",
    "\n",
    "    adp_month = (\n",
    "        m.groupby(\n",
    "            [\"season\",\"start_month\",\"dynasty_class\",\"type\",\"md_scoring_type\",\"st_teams\",\"st_rounds\",\"is_superflex\",\"player_id\"],\n",
    "            dropna=False\n",
    "        )\n",
    "        .agg(\n",
    "            drafts=(\"draft_id\",\"nunique\"),\n",
    "            picks=(\"pick_no\",\"size\"),\n",
    "            adp=(\"pick_no\",\"mean\"),\n",
    "            min_pick=(\"pick_no\",\"min\"),\n",
    "            max_pick=(\"pick_no\",\"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    adp_month[\"adp\"] = adp_month[\"adp\"].round(2)\n",
    "\n",
    "    adp_season = (\n",
    "        m.groupby(\n",
    "            [\"season\",\"dynasty_class\",\"type\",\"md_scoring_type\",\"st_teams\",\"st_rounds\",\"is_superflex\",\"player_id\"],\n",
    "            dropna=False\n",
    "        )\n",
    "        .agg(\n",
    "            drafts=(\"draft_id\",\"nunique\"),\n",
    "            picks=(\"pick_no\",\"size\"),\n",
    "            adp=(\"pick_no\",\"mean\"),\n",
    "            min_pick=(\"pick_no\",\"min\"),\n",
    "            max_pick=(\"pick_no\",\"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    adp_season[\"adp\"] = adp_season[\"adp\"].round(2)\n",
    "    adp_season[\"start_month\"] = \"ALL\"\n",
    "\n",
    "    return pd.concat([adp_month, adp_season], ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# RUN UPDATE\n",
    "# ----------------------------\n",
    "season = CURRENT_SEASON\n",
    "leagues_df, league_users_df = discover_leagues(season, SEED_USERS)\n",
    "\n",
    "leagues_df.to_parquet(os.path.join(RAW_DIR, \"leagues\", f\"leagues_{season}.parquet\"), index=False)\n",
    "league_users_df.to_parquet(os.path.join(RAW_DIR, \"league_users\", f\"league_users_{season}.parquet\"), index=False)\n",
    "\n",
    "if leagues_df.empty:\n",
    "    raise RuntimeError(\"No leagues discovered; cannot update.\")\n",
    "\n",
    "league_ids = leagues_df[\"league_id\"].astype(str).unique().tolist()\n",
    "drafts_df = fetch_drafts_for_leagues(league_ids, season)\n",
    "drafts_df.to_parquet(os.path.join(RAW_DIR, \"drafts\", f\"drafts_{season}.parquet\"), index=False)\n",
    "\n",
    "draft_catalog = build_draft_catalog(drafts_df)\n",
    "\n",
    "picks_df = fetch_picks_for_completed_drafts(drafts_df, season)\n",
    "picks_df.to_parquet(os.path.join(RAW_DIR, \"picks\", f\"picks_{season}.parquet\"), index=False)\n",
    "\n",
    "adp_ts = compute_adp_time_series(picks_df, draft_catalog)\n",
    "\n",
    "out_dir = os.path.join(SNAP_DIR, \"adp_time_series\", f\"season={season}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "adp_ts.to_parquet(os.path.join(out_dir, \"adp_time_series.parquet\"), index=False)\n",
    "\n",
    "cat_dir = os.path.join(SNAP_DIR, \"draft_catalog\", f\"season={season}\")\n",
    "os.makedirs(cat_dir, exist_ok=True)\n",
    "draft_catalog.to_parquet(os.path.join(cat_dir, \"draft_catalog.parquet\"), index=False)\n",
    "\n",
    "print(\"[OK] Updated ADP time-series:\", os.path.join(out_dir, \"adp_time_series.parquet\"), adp_ts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104be41-f177-4436-a311-a425ce97b75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
