{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ceed57-afde-4a6f-81cc-50f38ae75d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [2023] DISCOVERY STEP 0 | users=5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] leagues chunk 1 (5): 100%|████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Leagues fetched=172 | new=172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] league users chunk 1 (172): 100%|████████████████████████████████████████████| 172/172 [00:00<00:00, 454.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Next frontier users=1280 | total users seen=1285\n",
      "\n",
      "=== [2023] DISCOVERY STEP 1 | users=1280 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] leagues chunk 1 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 264.53it/s]\n",
      "[2023] leagues chunk 2 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 346.82it/s]\n",
      "[2023] leagues chunk 3 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 287.34it/s]\n",
      "[2023] leagues chunk 4 (80): 100%|█████████████████████████████████████████████████████| 80/80 [00:01<00:00, 66.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] Leagues fetched=17407 | new=17237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] league users chunk 1 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 531.86it/s]\n",
      "[2023] league users chunk 2 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 421.60it/s]\n",
      "[2023] league users chunk 3 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 512.99it/s]\n",
      "[2023] league users chunk 4 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 480.83it/s]\n",
      "[2023] league users chunk 5 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 529.41it/s]\n",
      "[2023] league users chunk 6 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 513.21it/s]\n",
      "[2023] league users chunk 7 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 510.24it/s]\n",
      "[2023] league users chunk 8 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 484.11it/s]\n",
      "[2023] league users chunk 9 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 435.16it/s]\n",
      "[2023] league users chunk 10 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 529.46it/s]\n",
      "[2023] league users chunk 11 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 443.12it/s]\n",
      "[2023] league users chunk 12 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 500.09it/s]\n",
      "[2023] league users chunk 13 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 499.72it/s]\n",
      "[2023] league users chunk 14 (400): 100%|███████████████████████████████████████████| 400/400 [00:01<00:00, 397.90it/s]\n",
      "[2023] league users chunk 15 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 427.17it/s]\n",
      "[2023] league users chunk 16 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 434.52it/s]\n",
      "[2023] league users chunk 17 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 495.44it/s]\n",
      "[2023] league users chunk 18 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 487.42it/s]\n",
      "[2023] league users chunk 19 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 460.93it/s]\n",
      "[2023] league users chunk 20 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 401.69it/s]\n",
      "[2023] league users chunk 21 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 481.23it/s]\n",
      "[2023] league users chunk 22 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 513.94it/s]\n",
      "[2023] league users chunk 23 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 524.39it/s]\n",
      "[2023] league users chunk 24 (400): 100%|████████████████████████████████████████████| 400/400 [00:04<00:00, 90.42it/s]\n",
      "[2023] league users chunk 25 (400): 100%|███████████████████████████████████████████| 400/400 [00:03<00:00, 118.14it/s]\n",
      "[2023] league users chunk 26 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 481.17it/s]\n",
      "[2023] league users chunk 27 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 467.30it/s]\n",
      "[2023] league users chunk 28 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 525.47it/s]\n",
      "[2023] league users chunk 29 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 532.87it/s]\n",
      "[2023] league users chunk 30 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 517.83it/s]\n",
      "[2023] league users chunk 31 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 478.20it/s]\n",
      "[2023] league users chunk 32 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 525.04it/s]\n",
      "[2023] league users chunk 33 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 492.04it/s]\n",
      "[2023] league users chunk 34 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 504.84it/s]\n",
      "[2023] league users chunk 35 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 458.59it/s]\n",
      "[2023] league users chunk 36 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 434.59it/s]\n",
      "[2023] league users chunk 37 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 439.40it/s]\n",
      "[2023] league users chunk 38 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 444.18it/s]\n",
      "[2023] league users chunk 39 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 512.66it/s]\n",
      "[2023] league users chunk 40 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 501.30it/s]\n",
      "[2023] league users chunk 41 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 463.84it/s]\n",
      "[2023] league users chunk 42 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 425.25it/s]\n",
      "[2023] league users chunk 43 (400): 100%|███████████████████████████████████████████| 400/400 [00:00<00:00, 511.63it/s]\n",
      "[2023] league users chunk 44 (37): 100%|██████████████████████████████████████████████| 37/37 [00:00<00:00, 269.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] leagues=(17409, 284) users=(205344, 80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] drafts chunk 1 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 509.91it/s]\n",
      "[2023] drafts chunk 2 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 419.43it/s]\n",
      "[2023] drafts chunk 3 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:04<00:00, 98.31it/s]\n",
      "[2023] drafts chunk 4 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 513.66it/s]\n",
      "[2023] drafts chunk 5 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 524.49it/s]\n",
      "[2023] drafts chunk 6 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 516.19it/s]\n",
      "[2023] drafts chunk 7 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 515.46it/s]\n",
      "[2023] drafts chunk 8 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 527.72it/s]\n",
      "[2023] drafts chunk 9 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 383.85it/s]\n",
      "[2023] drafts chunk 10 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 515.36it/s]\n",
      "[2023] drafts chunk 11 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 538.50it/s]\n",
      "[2023] drafts chunk 12 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 555.50it/s]\n",
      "[2023] drafts chunk 13 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 549.99it/s]\n",
      "[2023] drafts chunk 14 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 464.09it/s]\n",
      "[2023] drafts chunk 15 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 519.61it/s]\n",
      "[2023] drafts chunk 16 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 506.23it/s]\n",
      "[2023] drafts chunk 17 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 444.31it/s]\n",
      "[2023] drafts chunk 18 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 434.74it/s]\n",
      "[2023] drafts chunk 19 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 540.90it/s]\n",
      "[2023] drafts chunk 20 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 546.19it/s]\n",
      "[2023] drafts chunk 21 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 562.80it/s]\n",
      "[2023] drafts chunk 22 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 529.68it/s]\n",
      "[2023] drafts chunk 23 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 555.23it/s]\n",
      "[2023] drafts chunk 24 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 514.18it/s]\n",
      "[2023] drafts chunk 25 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 396.14it/s]\n",
      "[2023] drafts chunk 26 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 527.81it/s]\n",
      "[2023] drafts chunk 27 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 520.34it/s]\n",
      "[2023] drafts chunk 28 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 493.41it/s]\n",
      "[2023] drafts chunk 29 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 543.74it/s]\n",
      "[2023] drafts chunk 30 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 521.57it/s]\n",
      "[2023] drafts chunk 31 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 501.68it/s]\n",
      "[2023] drafts chunk 32 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 513.39it/s]\n",
      "[2023] drafts chunk 33 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 536.62it/s]\n",
      "[2023] drafts chunk 34 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 549.46it/s]\n",
      "[2023] drafts chunk 35 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 509.32it/s]\n",
      "[2023] drafts chunk 36 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 477.11it/s]\n",
      "[2023] drafts chunk 37 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 530.54it/s]\n",
      "[2023] drafts chunk 38 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 499.82it/s]\n",
      "[2023] drafts chunk 39 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 509.08it/s]\n",
      "[2023] drafts chunk 40 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 507.07it/s]\n",
      "[2023] drafts chunk 41 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 503.78it/s]\n",
      "[2023] drafts chunk 42 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 508.79it/s]\n",
      "[2023] drafts chunk 43 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 541.92it/s]\n",
      "[2023] drafts chunk 44 (209): 100%|█████████████████████████████████████████████████| 209/209 [00:00<00:00, 434.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] drafts=(18878, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023] picks chunk 1 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 296.55it/s]\n",
      "[2023] picks chunk 2 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:03<00:00, 122.57it/s]\n",
      "[2023] picks chunk 3 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 274.56it/s]\n",
      "[2023] picks chunk 4 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 288.04it/s]\n",
      "[2023] picks chunk 5 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 269.57it/s]\n",
      "[2023] picks chunk 6 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:02<00:00, 196.76it/s]\n",
      "[2023] picks chunk 7 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 259.49it/s]\n",
      "[2023] picks chunk 8 (400): 100%|███████████████████████████████████████████████████| 400/400 [00:01<00:00, 268.69it/s]\n",
      "[2023] picks chunk 9 (400): 100%|████████████████████████████████████████████████████| 400/400 [00:14<00:00, 27.25it/s]\n",
      "[2023] picks chunk 10 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 270.44it/s]\n",
      "[2023] picks chunk 11 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 200.85it/s]\n",
      "[2023] picks chunk 12 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 251.14it/s]\n",
      "[2023] picks chunk 13 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 234.71it/s]\n",
      "[2023] picks chunk 14 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 259.82it/s]\n",
      "[2023] picks chunk 15 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 247.05it/s]\n",
      "[2023] picks chunk 16 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 237.58it/s]\n",
      "[2023] picks chunk 17 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 240.84it/s]\n",
      "[2023] picks chunk 18 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:02<00:00, 192.16it/s]\n",
      "[2023] picks chunk 19 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 304.51it/s]\n",
      "[2023] picks chunk 20 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 290.37it/s]\n",
      "[2023] picks chunk 21 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 281.89it/s]\n",
      "[2023] picks chunk 22 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 267.01it/s]\n",
      "[2023] picks chunk 23 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 317.86it/s]\n",
      "[2023] picks chunk 24 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 294.09it/s]\n",
      "[2023] picks chunk 25 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 284.03it/s]\n",
      "[2023] picks chunk 26 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 290.26it/s]\n",
      "[2023] picks chunk 27 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 251.91it/s]\n",
      "[2023] picks chunk 28 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 297.68it/s]\n",
      "[2023] picks chunk 29 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 273.72it/s]\n",
      "[2023] picks chunk 30 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 276.06it/s]\n",
      "[2023] picks chunk 31 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 274.96it/s]\n",
      "[2023] picks chunk 32 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 256.64it/s]\n",
      "[2023] picks chunk 33 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 260.64it/s]\n",
      "[2023] picks chunk 34 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 256.95it/s]\n",
      "[2023] picks chunk 35 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 235.23it/s]\n",
      "[2023] picks chunk 36 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 256.04it/s]\n",
      "[2023] picks chunk 37 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 222.50it/s]\n",
      "[2023] picks chunk 38 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 236.02it/s]\n",
      "[2023] picks chunk 39 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 232.21it/s]\n",
      "[2023] picks chunk 40 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 257.99it/s]\n",
      "[2023] picks chunk 41 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 220.04it/s]\n",
      "[2023] picks chunk 42 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 234.53it/s]\n",
      "[2023] picks chunk 43 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 266.25it/s]\n",
      "[2023] picks chunk 44 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 251.74it/s]\n",
      "[2023] picks chunk 45 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:01<00:00, 210.81it/s]\n",
      "[2023] picks chunk 46 (322): 100%|██████████████████████████████████████████████████| 322/322 [00:01<00:00, 212.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023] picks=(3225757, 10)\n",
      "[2023] adp_ts=(480740, 14)\n",
      "[OK] wrote season=2023 -> sleeper_dynasty_adp\\data\\snapshots\\adp_time_series\\season=2023\n",
      "\n",
      "=== [2024] DISCOVERY STEP 0 | users=5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024] leagues chunk 1 (5): 100%|████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024] Leagues fetched=254 | new=254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024] league users chunk 1 (254): 100%|████████████████████████████████████████████| 254/254 [00:00<00:00, 486.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024] Next frontier users=1681 | total users seen=1686\n",
      "\n",
      "=== [2024] DISCOVERY STEP 1 | users=1681 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024] leagues chunk 1 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 247.94it/s]\n",
      "[2024] leagues chunk 2 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 348.78it/s]\n",
      "[2024] leagues chunk 3 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 277.50it/s]\n",
      "[2024] leagues chunk 4 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 224.13it/s]\n",
      "[2024] leagues chunk 5 (81): 100%|████████████████████████████████████████████████████| 81/81 [00:00<00:00, 169.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024] Leagues fetched=21409 | new=21157\n",
      "[2024] Hit MAX_LEAGUES_TOTAL cap.\n",
      "[2024] leagues=(21411, 284) users=(3132, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024] drafts chunk 1 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 525.53it/s]\n",
      "[2024] drafts chunk 2 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 526.64it/s]\n",
      "[2024] drafts chunk 3 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 501.35it/s]\n",
      "[2024] drafts chunk 4 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 558.60it/s]\n",
      "[2024] drafts chunk 5 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 503.25it/s]\n",
      "[2024] drafts chunk 6 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 472.17it/s]\n",
      "[2024] drafts chunk 7 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 510.57it/s]\n",
      "[2024] drafts chunk 8 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 543.67it/s]\n",
      "[2024] drafts chunk 9 (400): 100%|██████████████████████████████████████████████████| 400/400 [00:00<00:00, 519.91it/s]\n",
      "[2024] drafts chunk 10 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 534.01it/s]\n",
      "[2024] drafts chunk 11 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 510.06it/s]\n",
      "[2024] drafts chunk 12 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 520.35it/s]\n",
      "[2024] drafts chunk 13 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 526.49it/s]\n",
      "[2024] drafts chunk 14 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 538.49it/s]\n",
      "[2024] drafts chunk 15 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 541.93it/s]\n",
      "[2024] drafts chunk 16 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 554.95it/s]\n",
      "[2024] drafts chunk 17 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 530.55it/s]\n",
      "[2024] drafts chunk 18 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 458.95it/s]\n",
      "[2024] drafts chunk 19 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 510.42it/s]\n",
      "[2024] drafts chunk 20 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 478.04it/s]\n",
      "[2024] drafts chunk 21 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:03<00:00, 109.72it/s]\n",
      "[2024] drafts chunk 22 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 492.49it/s]\n",
      "[2024] drafts chunk 23 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 229.49it/s]\n",
      "[2024] drafts chunk 24 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 536.96it/s]\n",
      "[2024] drafts chunk 25 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 587.42it/s]\n",
      "[2024] drafts chunk 26 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 475.69it/s]\n",
      "[2024] drafts chunk 27 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 464.93it/s]\n",
      "[2024] drafts chunk 28 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 536.76it/s]\n",
      "[2024] drafts chunk 29 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 515.52it/s]\n",
      "[2024] drafts chunk 30 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 527.65it/s]\n",
      "[2024] drafts chunk 31 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 554.08it/s]\n",
      "[2024] drafts chunk 32 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 557.36it/s]\n",
      "[2024] drafts chunk 33 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 531.52it/s]\n",
      "[2024] drafts chunk 34 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 511.91it/s]\n",
      "[2024] drafts chunk 35 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 573.08it/s]\n",
      "[2024] drafts chunk 36 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 368.77it/s]\n",
      "[2024] drafts chunk 37 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 517.92it/s]\n",
      "[2024] drafts chunk 38 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 527.06it/s]\n",
      "[2024] drafts chunk 39 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 209.97it/s]\n",
      "[2024] drafts chunk 40 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 481.49it/s]\n",
      "[2024] drafts chunk 41 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 538.90it/s]\n",
      "[2024] drafts chunk 42 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 496.96it/s]\n",
      "[2024] drafts chunk 43 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 445.76it/s]\n",
      "[2024] drafts chunk 44 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 553.81it/s]\n",
      "[2024] drafts chunk 45 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 519.90it/s]\n",
      "[2024] drafts chunk 46 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 488.88it/s]\n",
      "[2024] drafts chunk 47 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 552.83it/s]\n",
      "[2024] drafts chunk 48 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 441.43it/s]\n",
      "[2024] drafts chunk 49 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 541.31it/s]\n",
      "[2024] drafts chunk 50 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 519.55it/s]\n",
      "[2024] drafts chunk 51 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 503.95it/s]\n",
      "[2024] drafts chunk 52 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 513.17it/s]\n",
      "[2024] drafts chunk 53 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:00<00:00, 480.72it/s]\n",
      "[2024] drafts chunk 54 (211): 100%|█████████████████████████████████████████████████| 211/211 [00:00<00:00, 456.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024] drafts=(22857, 15)\n"
     ]
    },
    {
     "ename": "FloatingPointError",
     "evalue": "overflow encountered in multiply",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 387\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# 3) Draft catalog (adds start_month time anchor)\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m draft_catalog \u001b[38;5;241m=\u001b[39m build_draft_catalog(drafts_df)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# 4) Picks\u001b[39;00m\n\u001b[0;32m    390\u001b[0m picks_df \u001b[38;5;241m=\u001b[39m fetch_picks_for_completed_drafts(drafts_df, season)\n",
      "Cell \u001b[1;32mIn[1], line 278\u001b[0m, in \u001b[0;36mbuild_draft_catalog\u001b[1;34m(drafts_df)\u001b[0m\n\u001b[0;32m    275\u001b[0m         df[c] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[c], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# time anchors (THIS is what we use for \"over time\")\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_dt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_dt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_month\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_dt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m convert_listlike(arg\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:407\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot specify both format and unit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_datetime_with_unit(arg, unit, name, utc, errors)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, datetime, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:512\u001b[0m, in \u001b[0;36m_to_datetime_with_unit\u001b[1;34m(arg, unit, name, utc, errors)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         arr \u001b[38;5;241m=\u001b[39m cast_from_unit_vectorized(arg, unit\u001b[38;5;241m=\u001b[39munit)\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m OutOfBoundsDatetime:\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mconversion.pyx:149\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3360\u001b[0m, in \u001b[0;36mround\u001b[1;34m(a, decimals, out)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_round_dispatcher)\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mround\u001b[39m(a, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3272\u001b[0m \u001b[38;5;124;03m    Evenly round to the given number of decimals.\u001b[39;00m\n\u001b[0;32m   3273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3358\u001b[0m \n\u001b[0;32m   3359\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m'\u001b[39m, decimals\u001b[38;5;241m=\u001b[39mdecimals, out\u001b[38;5;241m=\u001b[39mout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mFloatingPointError\u001b[0m: overflow encountered in multiply"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 01_ingest_historical.py (Jupyter-friendly)\n",
    "# - Backfills historical Sleeper drafts/picks and builds ADP time series\n",
    "# - Time axis is the DRAFT start_time (epoch ms), not file pull time\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Set\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SEASONS = [2023, 2024]     # <-- set your historical years here\n",
    "SEED_USERS = [\n",
    "    (\"camsnotsober\", \"567994319854673920\"),\n",
    "    (\"dynastybuck\", \"332066581859282944\"),\n",
    "    (\"curtistodd\", \"568256222760906752\"),\n",
    "    (\"elnostrathomas\", \"387839476958965760\"),\n",
    "    (\"coombesie9\", \"386648007942254592\"),\n",
    "]\n",
    "\n",
    "# discovery controls\n",
    "MAX_EXPANSION_STEPS = 1       # 0 = only seed users, 1 = one hop, etc.\n",
    "MAX_USERS_PER_STEP = 2500\n",
    "MAX_LEAGUES_TOTAL = 20000\n",
    "\n",
    "# request / parallelism controls\n",
    "MAX_WORKERS = 40\n",
    "CHUNK_SIZE = 400\n",
    "SLEEP_BETWEEN_CHUNKS_SEC = 8\n",
    "\n",
    "# filters for ADP (keep broad in 01; filter later in 03)\n",
    "KEEP_DYNASTY_CLASSES = {\"startup\", \"rookie\"}  # only build ADP TS for these\n",
    "\n",
    "ROOT_DIR = \"sleeper_dynasty_adp\"\n",
    "\n",
    "DIR_RAW_LEAGUES      = os.path.join(ROOT_DIR, \"data\", \"raw\", \"leagues\")\n",
    "DIR_RAW_LEAGUE_USERS = os.path.join(ROOT_DIR, \"data\", \"raw\", \"league_users\")\n",
    "DIR_RAW_DRAFTS       = os.path.join(ROOT_DIR, \"data\", \"raw\", \"drafts\")\n",
    "DIR_RAW_PICKS        = os.path.join(ROOT_DIR, \"data\", \"raw\", \"picks\")\n",
    "DIR_SNAP_ADP_TS      = os.path.join(ROOT_DIR, \"data\", \"snapshots\", \"adp_time_series\")\n",
    "DIR_SNAP_DRAFT_CAT   = os.path.join(ROOT_DIR, \"data\", \"snapshots\", \"draft_catalog\")\n",
    "\n",
    "for d in [\n",
    "    DIR_RAW_LEAGUES, DIR_RAW_LEAGUE_USERS, DIR_RAW_DRAFTS, DIR_RAW_PICKS,\n",
    "    DIR_SNAP_ADP_TS, DIR_SNAP_DRAFT_CAT\n",
    "]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HTTP\n",
    "# -----------------------------\n",
    "BASE = \"https://api.sleeper.app/v1\"\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Sleeper-Dynasty-ADP/1.0\"})\n",
    "\n",
    "\n",
    "def get_json(url: str, timeout: int = 30, retries: int = 4, backoff: float = 1.8) -> Any:\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=timeout)\n",
    "            if r.status_code == 429:\n",
    "                time.sleep(min(30, (backoff ** i) + 1))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(min(30, (backoff ** i) + 0.5))\n",
    "    raise RuntimeError(f\"GET failed: {url}\\nLast error: {last_err}\")\n",
    "\n",
    "\n",
    "def chunked(lst: List[Any], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def parallel_fetch(urls: List[str], desc: str) -> List[Tuple[str, Any, Optional[str]]]:\n",
    "    out: List[Tuple[str, Any, Optional[str]]] = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(get_json, u): u for u in urls}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=desc):\n",
    "            u = futs[fut]\n",
    "            try:\n",
    "                out.append((u, fut.result(), None))\n",
    "            except Exception as e:\n",
    "                out.append((u, None, str(e)))\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# URL helpers\n",
    "# -----------------------------\n",
    "def url_user_leagues(user_id: str, season: int) -> str:\n",
    "    return f\"{BASE}/user/{user_id}/leagues/nfl/{season}\"\n",
    "\n",
    "def url_league_users(league_id: str) -> str:\n",
    "    return f\"{BASE}/league/{league_id}/users\"\n",
    "\n",
    "def url_league_drafts(league_id: str) -> str:\n",
    "    return f\"{BASE}/league/{league_id}/drafts\"\n",
    "\n",
    "def url_draft_picks(draft_id: str) -> str:\n",
    "    return f\"{BASE}/draft/{draft_id}/picks\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SAFE TIME CONVERSION (fixes overflow)\n",
    "# -----------------------------\n",
    "def safe_ms_to_datetime_utc(ms_series: pd.Series, *, label: str = \"start_time\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert epoch-ms -> UTC datetime safely.\n",
    "    Masks out-of-range ms values so pandas/numpy never overflows internally.\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(ms_series, errors=\"coerce\")\n",
    "\n",
    "    # plausible bounds for Sleeper NFL drafts\n",
    "    lower = pd.Timestamp(\"2010-01-01\", tz=\"UTC\").value // 1_000_000  # ns -> ms\n",
    "    upper = pd.Timestamp(\"2036-12-31\", tz=\"UTC\").value // 1_000_000\n",
    "\n",
    "    bad = s.notna() & ((s < lower) | (s > upper))\n",
    "    bad_count = int(bad.sum())\n",
    "\n",
    "    if bad_count > 0:\n",
    "        examples = ms_series[bad].head(5).tolist()\n",
    "        print(f\"[warn] {label}: found {bad_count:,} out-of-range ms values. Examples: {examples}\")\n",
    "\n",
    "    s = s.mask(bad, np.nan)\n",
    "    return pd.to_datetime(s, unit=\"ms\", utc=True, errors=\"coerce\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DISCOVERY (leagues + league_users)\n",
    "# -----------------------------\n",
    "def fetch_leagues_for_users(user_ids: List[str], season: int, season_tag: str) -> pd.DataFrame:\n",
    "    urls = [url_user_leagues(uid, season) for uid in user_ids]\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] leagues chunk {i} ({len(chunk)})\")\n",
    "        for _u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            for lg in data:\n",
    "                lg[\"_season\"] = season\n",
    "                rows.append(lg)\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.json_normalize(rows).drop_duplicates(subset=[\"league_id\"])\n",
    "\n",
    "\n",
    "def fetch_users_for_leagues(league_ids: List[str], season_tag: str) -> pd.DataFrame:\n",
    "    urls = [url_league_users(lid) for lid in league_ids]\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] league users chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            league_id = u.split(\"/league/\")[1].split(\"/users\")[0]\n",
    "            for usr in data:\n",
    "                usr[\"_league_id\"] = league_id\n",
    "                rows.append(usr)\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "\n",
    "def discover_leagues(season: int, seed_users: List[Tuple[str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    season_tag = str(season)\n",
    "\n",
    "    frontier_users = [uid for _name, uid in seed_users]\n",
    "    seen_users: Set[str] = set(frontier_users)\n",
    "    seen_leagues: Set[str] = set()\n",
    "\n",
    "    leagues_parts: List[pd.DataFrame] = []\n",
    "    memberships_parts: List[pd.DataFrame] = []\n",
    "\n",
    "    for step in range(MAX_EXPANSION_STEPS + 1):\n",
    "        frontier_users = frontier_users[:MAX_USERS_PER_STEP]\n",
    "        print(f\"\\n=== [{season_tag}] DISCOVERY STEP {step} | users={len(frontier_users)} ===\")\n",
    "\n",
    "        leagues_df = fetch_leagues_for_users(frontier_users, season, season_tag)\n",
    "        if leagues_df.empty:\n",
    "            break\n",
    "\n",
    "        leagues_df[\"league_id\"] = leagues_df[\"league_id\"].astype(str)\n",
    "        new_leagues_df = leagues_df[~leagues_df[\"league_id\"].isin(seen_leagues)].copy()\n",
    "        print(f\"[{season_tag}] Leagues fetched={len(leagues_df):,} | new={len(new_leagues_df):,}\")\n",
    "\n",
    "        if new_leagues_df.empty:\n",
    "            break\n",
    "\n",
    "        leagues_parts.append(new_leagues_df)\n",
    "        new_league_ids = new_leagues_df[\"league_id\"].tolist()\n",
    "        seen_leagues.update(new_league_ids)\n",
    "\n",
    "        if len(seen_leagues) >= MAX_LEAGUES_TOTAL:\n",
    "            print(f\"[{season_tag}] Hit MAX_LEAGUES_TOTAL cap.\")\n",
    "            break\n",
    "\n",
    "        mem_df = fetch_users_for_leagues(new_league_ids, season_tag)\n",
    "        if not mem_df.empty:\n",
    "            memberships_parts.append(mem_df)\n",
    "\n",
    "        # stop expansion if no more steps\n",
    "        if step == MAX_EXPANSION_STEPS or mem_df.empty or \"user_id\" not in mem_df.columns:\n",
    "            break\n",
    "\n",
    "        discovered_users = mem_df[\"user_id\"].dropna().astype(str).unique().tolist()\n",
    "        frontier_users = [u for u in discovered_users if u not in seen_users]\n",
    "        seen_users.update(frontier_users)\n",
    "        print(f\"[{season_tag}] Next frontier users={len(frontier_users):,} | total users seen={len(seen_users):,}\")\n",
    "\n",
    "    leagues_out = pd.concat(leagues_parts, ignore_index=True) if leagues_parts else pd.DataFrame()\n",
    "    memberships_out = pd.concat(memberships_parts, ignore_index=True) if memberships_parts else pd.DataFrame()\n",
    "\n",
    "    # write raw\n",
    "    leagues_path = os.path.join(DIR_RAW_LEAGUES, f\"leagues_{season}.parquet\")\n",
    "    users_path  = os.path.join(DIR_RAW_LEAGUE_USERS, f\"league_users_{season}.parquet\")\n",
    "    leagues_out.to_parquet(leagues_path, index=False)\n",
    "    memberships_out.to_parquet(users_path, index=False)\n",
    "\n",
    "    print(f\"[{season_tag}] leagues={leagues_out.shape} users={memberships_out.shape}\")\n",
    "    return leagues_out, memberships_out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DRAFTS\n",
    "# -----------------------------\n",
    "def draft_to_row(d: dict, league_id: str, season: int) -> dict:\n",
    "    md = d.get(\"metadata\") or {}\n",
    "    st = d.get(\"settings\") or {}\n",
    "\n",
    "    return {\n",
    "        \"draft_id\": str(d.get(\"draft_id\") or \"\"),\n",
    "        \"league_id\": str(league_id),\n",
    "        \"season\": int(season),\n",
    "\n",
    "        \"draft_status\": d.get(\"status\"),\n",
    "        \"type\": d.get(\"type\"),          # snake / linear / auction\n",
    "        \"sport\": d.get(\"sport\"),\n",
    "        \"season_type\": d.get(\"season_type\"),\n",
    "\n",
    "        \"created\": d.get(\"created\"),\n",
    "        \"start_time\": d.get(\"start_time\"),\n",
    "        \"last_picked\": d.get(\"last_picked\"),\n",
    "\n",
    "        \"md_scoring_type\": md.get(\"scoring_type\"),  # includes dynasty_*\n",
    "        \"md_name\": md.get(\"name\"),\n",
    "        \"md_league_type\": md.get(\"league_type\"),\n",
    "\n",
    "        \"st_teams\": st.get(\"teams\"),\n",
    "        \"st_rounds\": st.get(\"rounds\"),\n",
    "        \"st_pick_timer\": st.get(\"pick_timer\"),\n",
    "        \"st_reversal_round\": st.get(\"reversal_round\"),\n",
    "\n",
    "        \"st_slots_qb\": st.get(\"slots_qb\"),\n",
    "        \"st_slots_rb\": st.get(\"slots_rb\"),\n",
    "        \"st_slots_wr\": st.get(\"slots_wr\"),\n",
    "        \"st_slots_te\": st.get(\"slots_te\"),\n",
    "        \"st_slots_flex\": st.get(\"slots_flex\"),\n",
    "        \"st_slots_super_flex\": st.get(\"slots_super_flex\"),\n",
    "        \"st_slots_def\": st.get(\"slots_def\"),\n",
    "        \"st_slots_k\": st.get(\"slots_k\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_drafts_for_leagues(league_ids: List[str], season: int) -> pd.DataFrame:\n",
    "    season_tag = str(season)\n",
    "    urls = [url_league_drafts(lid) for lid in league_ids]\n",
    "    parts, buf = [], []\n",
    "\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] drafts chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            league_id = u.split(\"/league/\")[1].split(\"/drafts\")[0]\n",
    "            for d in data:\n",
    "                row = draft_to_row(d, league_id, season)\n",
    "                if row[\"draft_id\"]:\n",
    "                    buf.append(row)\n",
    "\n",
    "        if buf:\n",
    "            parts.append(pd.DataFrame(buf).drop_duplicates(subset=[\"draft_id\"]))\n",
    "            buf = []\n",
    "\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    drafts_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    out_path = os.path.join(DIR_RAW_DRAFTS, f\"drafts_{season}.parquet\")\n",
    "    drafts_df.to_parquet(out_path, index=False)\n",
    "    print(f\"[{season_tag}] drafts={drafts_df.shape}\")\n",
    "    return drafts_df\n",
    "\n",
    "\n",
    "def build_draft_catalog(drafts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = drafts_df.copy()\n",
    "\n",
    "    # normalize id cols\n",
    "    for c in [\"draft_id\", \"league_id\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "\n",
    "    # numeric\n",
    "    num_cols = [\n",
    "        \"created\", \"start_time\", \"last_picked\",\n",
    "        \"st_teams\", \"st_rounds\", \"st_pick_timer\", \"st_reversal_round\",\n",
    "        \"st_slots_qb\", \"st_slots_rb\", \"st_slots_wr\", \"st_slots_te\",\n",
    "        \"st_slots_flex\", \"st_slots_super_flex\", \"st_slots_def\", \"st_slots_k\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # SAFE time anchors (this is the key fix)\n",
    "    df[\"start_dt\"] = safe_ms_to_datetime_utc(df[\"start_time\"], label=\"start_time\")\n",
    "    df[\"start_date\"] = df[\"start_dt\"].dt.date.astype(\"string\")\n",
    "    df[\"start_month\"] = df[\"start_dt\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "    # flags\n",
    "    df[\"is_dynasty\"] = df[\"md_scoring_type\"].astype(str).str.contains(\"dynasty\", case=False, na=False)\n",
    "    df[\"is_superflex\"] = (df.get(\"st_slots_super_flex\", 0).fillna(0) > 0) | df[\"md_scoring_type\"].astype(str).str.contains(\"2qb|superflex\", case=False, na=False)\n",
    "\n",
    "    # dynasty class\n",
    "    def _dynasty_class(row) -> str:\n",
    "        if not bool(row.get(\"is_dynasty\", False)):\n",
    "            return \"non_dynasty\"\n",
    "        rounds = row.get(\"st_rounds\", np.nan)\n",
    "        if pd.notna(rounds) and rounds <= 6:\n",
    "            return \"rookie\"\n",
    "        if pd.notna(rounds) and rounds >= 14:\n",
    "            return \"startup\"\n",
    "        return \"other\"\n",
    "\n",
    "    df[\"dynasty_class\"] = df.apply(_dynasty_class, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PICKS\n",
    "# -----------------------------\n",
    "def pick_to_row(p: dict, draft_id: str) -> dict:\n",
    "    md = p.get(\"metadata\") or {}\n",
    "    return {\n",
    "        \"draft_id\": str(draft_id),\n",
    "        \"player_id\": p.get(\"player_id\"),\n",
    "        \"pick_no\": p.get(\"pick_no\"),\n",
    "        \"round\": p.get(\"round\"),\n",
    "        \"draft_slot\": p.get(\"draft_slot\"),\n",
    "        \"is_keeper\": p.get(\"is_keeper\"),\n",
    "        \"md_first_name\": md.get(\"first_name\"),\n",
    "        \"md_last_name\": md.get(\"last_name\"),\n",
    "        \"md_team\": md.get(\"team\"),\n",
    "        \"md_pos\": md.get(\"position\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_picks_for_completed_drafts(draft_catalog: pd.DataFrame, season: int) -> pd.DataFrame:\n",
    "    season_tag = str(season)\n",
    "\n",
    "    # robust status column\n",
    "    status_col = \"draft_status\" if \"draft_status\" in draft_catalog.columns else \"status\"\n",
    "    completed_ids = (\n",
    "        draft_catalog.loc[draft_catalog[status_col].astype(str).str.lower() == \"complete\", \"draft_id\"]\n",
    "        .astype(str).dropna().unique().tolist()\n",
    "    )\n",
    "\n",
    "    urls = [url_draft_picks(did) for did in completed_ids]\n",
    "    parts, buf = [], []\n",
    "\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] picks chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            draft_id = u.split(\"/draft/\")[1].split(\"/picks\")[0]\n",
    "            for p in data:\n",
    "                buf.append(pick_to_row(p, draft_id))\n",
    "\n",
    "        if buf:\n",
    "            parts.append(pd.DataFrame(buf))\n",
    "            buf = []\n",
    "\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    picks_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    out_path = os.path.join(DIR_RAW_PICKS, f\"picks_{season}.parquet\")\n",
    "    picks_df.to_parquet(out_path, index=False)\n",
    "    print(f\"[{season_tag}] picks={picks_df.shape}\")\n",
    "    return picks_df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ADP TIME SERIES (month anchored)\n",
    "# -----------------------------\n",
    "def compute_adp_time_series(picks_df: pd.DataFrame, draft_catalog: pd.DataFrame) -> pd.DataFrame:\n",
    "    # types\n",
    "    p = picks_df.copy()\n",
    "    p[\"draft_id\"] = p[\"draft_id\"].astype(str)\n",
    "    p[\"player_id\"] = p[\"player_id\"].astype(str)\n",
    "    p[\"pick_no\"] = pd.to_numeric(p[\"pick_no\"], errors=\"coerce\")\n",
    "\n",
    "    d = draft_catalog.copy()\n",
    "    d[\"draft_id\"] = d[\"draft_id\"].astype(str)\n",
    "\n",
    "    # merge in draft fields including start_month\n",
    "    keep_cols = [\n",
    "        \"draft_id\", \"season\", \"start_dt\", \"start_month\",\n",
    "        \"dynasty_class\", \"type\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\",\n",
    "        \"draft_status\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in d.columns]\n",
    "    m = p.merge(d[keep_cols], on=\"draft_id\", how=\"left\")\n",
    "\n",
    "    # clean\n",
    "    m = m[m[\"pick_no\"].notna() & m[\"player_id\"].notna()].copy()\n",
    "    m = m[m[\"start_month\"].notna()].copy()\n",
    "\n",
    "    # limit to dynasty classes of interest\n",
    "    m = m[m[\"dynasty_class\"].isin(list(KEEP_DYNASTY_CLASSES))].copy()\n",
    "\n",
    "    # aggregate by player_id + format + month\n",
    "    out = (\n",
    "        m.groupby(\n",
    "            [\"season\", \"start_month\", \"player_id\", \"dynasty_class\", \"type\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\"],\n",
    "            dropna=False\n",
    "        )\n",
    "        .agg(\n",
    "            drafts=(\"draft_id\", \"nunique\"),\n",
    "            picks=(\"pick_no\", \"size\"),\n",
    "            adp=(\"pick_no\", \"mean\"),\n",
    "            min_pick=(\"pick_no\", \"min\"),\n",
    "            max_pick=(\"pick_no\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    out[\"adp\"] = out[\"adp\"].round(2)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN LOOP (per season)\n",
    "# -----------------------------\n",
    "all_adp_parts = []\n",
    "all_draftcat_parts = []\n",
    "\n",
    "for season in SEASONS:\n",
    "    season_tag = str(season)\n",
    "\n",
    "    # 1) Discover leagues/users\n",
    "    leagues_df, league_users_df = discover_leagues(season, SEED_USERS)\n",
    "    if leagues_df.empty:\n",
    "        print(f\"[{season_tag}] No leagues discovered. Skipping season.\")\n",
    "        continue\n",
    "\n",
    "    league_ids = leagues_df[\"league_id\"].astype(str).unique().tolist()\n",
    "\n",
    "    # 2) Drafts\n",
    "    drafts_df = fetch_drafts_for_leagues(league_ids, season)\n",
    "    if drafts_df.empty:\n",
    "        print(f\"[{season_tag}] No drafts found. Skipping season.\")\n",
    "        continue\n",
    "\n",
    "    # 3) Draft catalog (safe start_time -> start_dt/start_month)\n",
    "    draft_catalog = build_draft_catalog(drafts_df)\n",
    "\n",
    "    # Save draft catalog snapshot for the season\n",
    "    cat_out_dir = os.path.join(DIR_SNAP_DRAFT_CAT, f\"season={season}\")\n",
    "    os.makedirs(cat_out_dir, exist_ok=True)\n",
    "    draft_catalog.to_parquet(os.path.join(cat_out_dir, \"draft_catalog.parquet\"), index=False)\n",
    "\n",
    "    # 4) Picks (completed drafts only)\n",
    "    picks_df = fetch_picks_for_completed_drafts(draft_catalog, season)\n",
    "    if picks_df.empty:\n",
    "        print(f\"[{season_tag}] No picks found. Skipping ADP TS.\")\n",
    "        continue\n",
    "\n",
    "    # 5) ADP time series by month (time anchored on start_time)\n",
    "    adp_ts = compute_adp_time_series(picks_df, draft_catalog)\n",
    "\n",
    "    # Save season partition\n",
    "    out_dir = os.path.join(DIR_SNAP_ADP_TS, f\"season={season}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    adp_ts.to_parquet(os.path.join(out_dir, \"adp_time_series.parquet\"), index=False)\n",
    "\n",
    "    print(f\"[{season_tag}] adp_ts={adp_ts.shape}\")\n",
    "    print(f\"[OK] wrote season={season} -> {out_dir}\")\n",
    "\n",
    "    all_adp_parts.append(adp_ts)\n",
    "    all_draftcat_parts.append(draft_catalog)\n",
    "\n",
    "# optional: combined outputs across seasons\n",
    "if all_adp_parts:\n",
    "    adp_all = pd.concat(all_adp_parts, ignore_index=True)\n",
    "    adp_all_path = os.path.join(DIR_SNAP_ADP_TS, \"adp_time_series_ALL.parquet\")\n",
    "    adp_all.to_parquet(adp_all_path, index=False)\n",
    "    print(f\"[OK] wrote combined ADP TS -> {adp_all_path} | shape={adp_all.shape}\")\n",
    "\n",
    "if all_draftcat_parts:\n",
    "    dc_all = pd.concat(all_draftcat_parts, ignore_index=True)\n",
    "    dc_all_path = os.path.join(DIR_SNAP_DRAFT_CAT, \"draft_catalog_ALL.parquet\")\n",
    "    dc_all.to_parquet(dc_all_path, index=False)\n",
    "    print(f\"[OK] wrote combined draft catalog -> {dc_all_path} | shape={dc_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896b287-7d6b-4f92-b19e-de55436aafa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
