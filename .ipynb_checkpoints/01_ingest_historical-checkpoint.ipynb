{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ceed57-afde-4a6f-81cc-50f38ae75d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [2026] DISCOVERY STEP 0 | users=5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026] leagues chunk 1 (5): 100%|████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026] Leagues fetched=83 | new=83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026] league users chunk 1 (83): 100%|███████████████████████████████████████████████| 83/83 [00:00<00:00, 176.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026] Next frontier users=720 | total users seen=725\n",
      "\n",
      "=== [2026] DISCOVERY STEP 1 | users=720 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026] leagues chunk 1 (400): 100%|█████████████████████████████████████████████████| 400/400 [00:01<00:00, 204.59it/s]\n",
      "[2026] leagues chunk 2 (320): 100%|█████████████████████████████████████████████████| 320/320 [00:01<00:00, 265.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026] Leagues fetched=6,077 | new=5,994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026] league users chunk 1 (400): 100%|█████████████████████████████████████████████| 400/400 [00:06<00:00, 66.60it/s]\n",
      "[2026] league users chunk 2 (400): 100%|████████████████████████████████████████████| 400/400 [00:00<00:00, 408.16it/s]\n",
      "[2026] league users chunk 3 (400): 100%|████████████████████████████████████████████| 400/400 [00:02<00:00, 179.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 01_ingest_historical.py (Jupyter-friendly)  ✅ FULL CORRECTED\n",
    "# - Backfills historical Sleeper drafts/picks and builds:\n",
    "#     1) ADP time series (month anchored on draft start_time)\n",
    "#     2) Auction price time series (month anchored on draft start_time)\n",
    "# - Time axis is the DRAFT start_time (epoch ms), not file pull time\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Tuple, Set\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SEASONS = [2026]     # <-- set your historical years here\n",
    "SEED_USERS = [\n",
    "    (\"camsnotsober\", \"567994319854673920\"),\n",
    "    (\"dynastybuck\", \"332066581859282944\"),\n",
    "    (\"curtistodd\", \"568256222760906752\"),\n",
    "    (\"elnostrathomas\", \"387839476958965760\"),\n",
    "    (\"coombesie9\", \"386648007942254592\"),\n",
    "]\n",
    "\n",
    "# discovery controls\n",
    "MAX_EXPANSION_STEPS = 2       # 0 = only seed users, 1 = one hop, etc.\n",
    "MAX_USERS_PER_STEP = 2500\n",
    "MAX_LEAGUES_TOTAL = 20000\n",
    "\n",
    "# request / parallelism controls\n",
    "MAX_WORKERS = 40\n",
    "CHUNK_SIZE = 400\n",
    "SLEEP_BETWEEN_CHUNKS_SEC = 8\n",
    "\n",
    "# filters for ADP/Auction (keep broad in 01; filter later in app)\n",
    "KEEP_DYNASTY_CLASSES = {\"startup\", \"rookie\"}  # only build time series for these\n",
    "\n",
    "ROOT_DIR = \"sleeper_dynasty_adp\"\n",
    "\n",
    "DIR_RAW_LEAGUES      = os.path.join(ROOT_DIR, \"data\", \"raw\", \"leagues\")\n",
    "DIR_RAW_LEAGUE_USERS = os.path.join(ROOT_DIR, \"data\", \"raw\", \"league_users\")\n",
    "DIR_RAW_DRAFTS       = os.path.join(ROOT_DIR, \"data\", \"raw\", \"drafts\")\n",
    "DIR_RAW_PICKS        = os.path.join(ROOT_DIR, \"data\", \"raw\", \"picks\")\n",
    "\n",
    "DIR_SNAP_ADP_TS      = os.path.join(ROOT_DIR, \"data\", \"snapshots\", \"adp_time_series\")\n",
    "DIR_SNAP_DRAFT_CAT   = os.path.join(ROOT_DIR, \"data\", \"snapshots\", \"draft_catalog\")\n",
    "\n",
    "# ✅ NEW: Auction snapshots (mirrors ADP structure)\n",
    "DIR_SNAP_AUCTION_TS  = os.path.join(ROOT_DIR, \"data\", \"snapshots\", \"auction_price_series\")\n",
    "DIR_SNAP_AUCTION_CAT = os.path.join(ROOT_DIR, \"data\", \"snapshots\", \"auction_draft_catalog\")\n",
    "\n",
    "for d in [\n",
    "    DIR_RAW_LEAGUES, DIR_RAW_LEAGUE_USERS, DIR_RAW_DRAFTS, DIR_RAW_PICKS,\n",
    "    DIR_SNAP_ADP_TS, DIR_SNAP_DRAFT_CAT,\n",
    "    DIR_SNAP_AUCTION_TS, DIR_SNAP_AUCTION_CAT,\n",
    "]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HTTP\n",
    "# -----------------------------\n",
    "BASE = \"https://api.sleeper.app/v1\"\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Sleeper-Dynasty-ADP/1.0\"})\n",
    "\n",
    "\n",
    "def get_json(url: str, timeout: int = 30, retries: int = 4, backoff: float = 1.8) -> Any:\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=timeout)\n",
    "            if r.status_code == 429:\n",
    "                time.sleep(min(30, (backoff ** i) + 1))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(min(30, (backoff ** i) + 0.5))\n",
    "    raise RuntimeError(f\"GET failed: {url}\\nLast error: {last_err}\")\n",
    "\n",
    "\n",
    "def chunked(lst: List[Any], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def parallel_fetch(urls: List[str], desc: str) -> List[Tuple[str, Any, Optional[str]]]:\n",
    "    out: List[Tuple[str, Any, Optional[str]]] = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(get_json, u): u for u in urls}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=desc):\n",
    "            u = futs[fut]\n",
    "            try:\n",
    "                out.append((u, fut.result(), None))\n",
    "            except Exception as e:\n",
    "                out.append((u, None, str(e)))\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# URL helpers\n",
    "# -----------------------------\n",
    "def url_user_leagues(user_id: str, season: int) -> str:\n",
    "    return f\"{BASE}/user/{user_id}/leagues/nfl/{season}\"\n",
    "\n",
    "def url_league_users(league_id: str) -> str:\n",
    "    return f\"{BASE}/league/{league_id}/users\"\n",
    "\n",
    "def url_league_drafts(league_id: str) -> str:\n",
    "    return f\"{BASE}/league/{league_id}/drafts\"\n",
    "\n",
    "def url_draft_picks(draft_id: str) -> str:\n",
    "    return f\"{BASE}/draft/{draft_id}/picks\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SAFE TIME CONVERSION (fixes overflow)\n",
    "# -----------------------------\n",
    "def safe_ms_to_datetime_utc(ms_series: pd.Series, *, label: str = \"start_time\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert epoch-ms -> UTC datetime safely.\n",
    "    Masks out-of-range ms values so pandas/numpy never overflows internally.\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(ms_series, errors=\"coerce\")\n",
    "\n",
    "    # plausible bounds for Sleeper NFL drafts\n",
    "    lower = pd.Timestamp(\"2010-01-01\", tz=\"UTC\").value // 1_000_000  # ns -> ms\n",
    "    upper = pd.Timestamp(\"2036-12-31\", tz=\"UTC\").value // 1_000_000\n",
    "\n",
    "    bad = s.notna() & ((s < lower) | (s > upper))\n",
    "    bad_count = int(bad.sum())\n",
    "\n",
    "    if bad_count > 0:\n",
    "        examples = ms_series[bad].head(5).tolist()\n",
    "        print(f\"[warn] {label}: found {bad_count:,} out-of-range ms values. Examples: {examples}\")\n",
    "\n",
    "    s = s.mask(bad, np.nan)\n",
    "    return pd.to_datetime(s, unit=\"ms\", utc=True, errors=\"coerce\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DISCOVERY (leagues + league_users)\n",
    "# -----------------------------\n",
    "def fetch_leagues_for_users(user_ids: List[str], season: int, season_tag: str) -> pd.DataFrame:\n",
    "    urls = [url_user_leagues(uid, season) for uid in user_ids]\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] leagues chunk {i} ({len(chunk)})\")\n",
    "        for _u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            for lg in data:\n",
    "                lg[\"_season\"] = season\n",
    "                rows.append(lg)\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.json_normalize(rows).drop_duplicates(subset=[\"league_id\"])\n",
    "\n",
    "\n",
    "def fetch_users_for_leagues(league_ids: List[str], season_tag: str) -> pd.DataFrame:\n",
    "    urls = [url_league_users(lid) for lid in league_ids]\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] league users chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            league_id = u.split(\"/league/\")[1].split(\"/users\")[0]\n",
    "            for usr in data:\n",
    "                usr[\"_league_id\"] = league_id\n",
    "                rows.append(usr)\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "\n",
    "def discover_leagues(season: int, seed_users: List[Tuple[str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    season_tag = str(season)\n",
    "\n",
    "    frontier_users = [uid for _name, uid in seed_users]\n",
    "    seen_users: Set[str] = set(frontier_users)\n",
    "    seen_leagues: Set[str] = set()\n",
    "\n",
    "    leagues_parts: List[pd.DataFrame] = []\n",
    "    memberships_parts: List[pd.DataFrame] = []\n",
    "\n",
    "    for step in range(MAX_EXPANSION_STEPS + 1):\n",
    "        frontier_users = frontier_users[:MAX_USERS_PER_STEP]\n",
    "        print(f\"\\n=== [{season_tag}] DISCOVERY STEP {step} | users={len(frontier_users)} ===\")\n",
    "\n",
    "        leagues_df = fetch_leagues_for_users(frontier_users, season, season_tag)\n",
    "        if leagues_df.empty:\n",
    "            break\n",
    "\n",
    "        leagues_df[\"league_id\"] = leagues_df[\"league_id\"].astype(str)\n",
    "        new_leagues_df = leagues_df[~leagues_df[\"league_id\"].isin(seen_leagues)].copy()\n",
    "        print(f\"[{season_tag}] Leagues fetched={len(leagues_df):,} | new={len(new_leagues_df):,}\")\n",
    "\n",
    "        if new_leagues_df.empty:\n",
    "            break\n",
    "\n",
    "        leagues_parts.append(new_leagues_df)\n",
    "        new_league_ids = new_leagues_df[\"league_id\"].tolist()\n",
    "        seen_leagues.update(new_league_ids)\n",
    "\n",
    "        if len(seen_leagues) >= MAX_LEAGUES_TOTAL:\n",
    "            print(f\"[{season_tag}] Hit MAX_LEAGUES_TOTAL cap.\")\n",
    "            break\n",
    "\n",
    "        mem_df = fetch_users_for_leagues(new_league_ids, season_tag)\n",
    "        if not mem_df.empty:\n",
    "            memberships_parts.append(mem_df)\n",
    "\n",
    "        # stop expansion if no more steps\n",
    "        if step == MAX_EXPANSION_STEPS or mem_df.empty or \"user_id\" not in mem_df.columns:\n",
    "            break\n",
    "\n",
    "        discovered_users = mem_df[\"user_id\"].dropna().astype(str).unique().tolist()\n",
    "        frontier_users = [u for u in discovered_users if u not in seen_users]\n",
    "        seen_users.update(frontier_users)\n",
    "        print(f\"[{season_tag}] Next frontier users={len(frontier_users):,} | total users seen={len(seen_users):,}\")\n",
    "\n",
    "    leagues_out = pd.concat(leagues_parts, ignore_index=True) if leagues_parts else pd.DataFrame()\n",
    "    memberships_out = pd.concat(memberships_parts, ignore_index=True) if memberships_parts else pd.DataFrame()\n",
    "\n",
    "    # write raw\n",
    "    leagues_path = os.path.join(DIR_RAW_LEAGUES, f\"leagues_{season}.parquet\")\n",
    "    users_path  = os.path.join(DIR_RAW_LEAGUE_USERS, f\"league_users_{season}.parquet\")\n",
    "    leagues_out.to_parquet(leagues_path, index=False)\n",
    "    memberships_out.to_parquet(users_path, index=False)\n",
    "\n",
    "    print(f\"[{season_tag}] leagues={leagues_out.shape} users={memberships_out.shape}\")\n",
    "    return leagues_out, memberships_out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DRAFTS\n",
    "# -----------------------------\n",
    "def draft_to_row(d: dict, league_id: str, season: int) -> dict:\n",
    "    md = d.get(\"metadata\") or {}\n",
    "    st = d.get(\"settings\") or {}\n",
    "\n",
    "    return {\n",
    "        \"draft_id\": str(d.get(\"draft_id\") or \"\"),\n",
    "        \"league_id\": str(league_id),\n",
    "        \"season\": int(season),\n",
    "\n",
    "        \"draft_status\": d.get(\"status\"),\n",
    "        \"type\": d.get(\"type\"),          # snake / linear / auction\n",
    "        \"sport\": d.get(\"sport\"),\n",
    "        \"season_type\": d.get(\"season_type\"),\n",
    "\n",
    "        \"created\": d.get(\"created\"),\n",
    "        \"start_time\": d.get(\"start_time\"),\n",
    "        \"last_picked\": d.get(\"last_picked\"),\n",
    "\n",
    "        \"md_scoring_type\": md.get(\"scoring_type\"),  # includes dynasty_*\n",
    "        \"md_name\": md.get(\"name\"),\n",
    "        \"md_league_type\": md.get(\"league_type\"),\n",
    "\n",
    "        \"st_teams\": st.get(\"teams\"),\n",
    "        \"st_rounds\": st.get(\"rounds\"),\n",
    "        \"st_pick_timer\": st.get(\"pick_timer\"),\n",
    "        \"st_reversal_round\": st.get(\"reversal_round\"),\n",
    "\n",
    "        \"st_slots_qb\": st.get(\"slots_qb\"),\n",
    "        \"st_slots_rb\": st.get(\"slots_rb\"),\n",
    "        \"st_slots_wr\": st.get(\"slots_wr\"),\n",
    "        \"st_slots_te\": st.get(\"slots_te\"),\n",
    "        \"st_slots_flex\": st.get(\"slots_flex\"),\n",
    "        \"st_slots_super_flex\": st.get(\"slots_super_flex\"),\n",
    "        \"st_slots_def\": st.get(\"slots_def\"),\n",
    "        \"st_slots_k\": st.get(\"slots_k\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_drafts_for_leagues(league_ids: List[str], season: int) -> pd.DataFrame:\n",
    "    season_tag = str(season)\n",
    "    urls = [url_league_drafts(lid) for lid in league_ids]\n",
    "    parts, buf = [], []\n",
    "\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] drafts chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            league_id = u.split(\"/league/\")[1].split(\"/drafts\")[0]\n",
    "            for d in data:\n",
    "                row = draft_to_row(d, league_id, season)\n",
    "                if row[\"draft_id\"]:\n",
    "                    buf.append(row)\n",
    "\n",
    "        if buf:\n",
    "            parts.append(pd.DataFrame(buf).drop_duplicates(subset=[\"draft_id\"]))\n",
    "            buf = []\n",
    "\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    drafts_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    out_path = os.path.join(DIR_RAW_DRAFTS, f\"drafts_{season}.parquet\")\n",
    "    drafts_df.to_parquet(out_path, index=False)\n",
    "    print(f\"[{season_tag}] drafts={drafts_df.shape}\")\n",
    "    return drafts_df\n",
    "\n",
    "\n",
    "def build_draft_catalog(drafts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = drafts_df.copy()\n",
    "\n",
    "    # normalize id cols\n",
    "    for c in [\"draft_id\", \"league_id\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "\n",
    "    # numeric\n",
    "    num_cols = [\n",
    "        \"created\", \"start_time\", \"last_picked\",\n",
    "        \"st_teams\", \"st_rounds\", \"st_pick_timer\", \"st_reversal_round\",\n",
    "        \"st_slots_qb\", \"st_slots_rb\", \"st_slots_wr\", \"st_slots_te\",\n",
    "        \"st_slots_flex\", \"st_slots_super_flex\", \"st_slots_def\", \"st_slots_k\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # SAFE time anchors\n",
    "    df[\"start_dt\"] = safe_ms_to_datetime_utc(df[\"start_time\"], label=\"start_time\")\n",
    "    df[\"start_date\"] = df[\"start_dt\"].dt.date.astype(\"string\")\n",
    "    df[\"start_month\"] = df[\"start_dt\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "    # flags\n",
    "    df[\"is_dynasty\"] = df[\"md_scoring_type\"].astype(str).str.contains(\"dynasty\", case=False, na=False)\n",
    "    df[\"is_superflex\"] = (df.get(\"st_slots_super_flex\", 0).fillna(0) > 0) | df[\"md_scoring_type\"].astype(str).str.contains(\n",
    "        \"2qb|superflex\", case=False, na=False\n",
    "    )\n",
    "\n",
    "    # dynasty class\n",
    "    def _dynasty_class(row) -> str:\n",
    "        if not bool(row.get(\"is_dynasty\", False)):\n",
    "            return \"non_dynasty\"\n",
    "        rounds = row.get(\"st_rounds\", np.nan)\n",
    "        if pd.notna(rounds) and rounds <= 6:\n",
    "            return \"rookie\"\n",
    "        if pd.notna(rounds) and rounds >= 14:\n",
    "            return \"startup\"\n",
    "        return \"other\"\n",
    "\n",
    "    df[\"dynasty_class\"] = df.apply(_dynasty_class, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ✅ NEW: auction-only catalog builder\n",
    "def build_auction_draft_catalog(draft_catalog: pd.DataFrame) -> pd.DataFrame:\n",
    "    if draft_catalog.empty or \"type\" not in draft_catalog.columns:\n",
    "        return pd.DataFrame()\n",
    "    return draft_catalog[draft_catalog[\"type\"].astype(str).str.lower() == \"auction\"].copy()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PICKS\n",
    "# -----------------------------\n",
    "def pick_to_row(p: dict, draft_id: str) -> dict:\n",
    "    md = p.get(\"metadata\") or {}\n",
    "    return {\n",
    "        \"draft_id\": str(draft_id),\n",
    "        \"player_id\": p.get(\"player_id\"),\n",
    "        \"pick_no\": p.get(\"pick_no\"),\n",
    "        \"round\": p.get(\"round\"),\n",
    "        \"draft_slot\": p.get(\"draft_slot\"),\n",
    "        \"is_keeper\": p.get(\"is_keeper\"),\n",
    "\n",
    "        \"md_first_name\": md.get(\"first_name\"),\n",
    "        \"md_last_name\": md.get(\"last_name\"),\n",
    "        \"md_team\": md.get(\"team\"),\n",
    "        \"md_pos\": md.get(\"position\"),\n",
    "\n",
    "        # ✅ NEW: auction price (exists for auction picks; null otherwise)\n",
    "        \"md_amount\": md.get(\"amount\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_picks_for_completed_drafts(draft_catalog: pd.DataFrame, season: int) -> pd.DataFrame:\n",
    "    season_tag = str(season)\n",
    "\n",
    "    # robust status column\n",
    "    status_col = \"draft_status\" if \"draft_status\" in draft_catalog.columns else \"status\"\n",
    "    completed_ids = (\n",
    "        draft_catalog.loc[draft_catalog[status_col].astype(str).str.lower() == \"complete\", \"draft_id\"]\n",
    "        .astype(str).dropna().unique().tolist()\n",
    "    )\n",
    "\n",
    "    urls = [url_draft_picks(did) for did in completed_ids]\n",
    "    parts, buf = [], []\n",
    "\n",
    "    for i, chunk in enumerate(chunked(urls, CHUNK_SIZE), start=1):\n",
    "        res = parallel_fetch(chunk, desc=f\"[{season_tag}] picks chunk {i} ({len(chunk)})\")\n",
    "        for u, data, err in res:\n",
    "            if err or data is None:\n",
    "                continue\n",
    "            draft_id = u.split(\"/draft/\")[1].split(\"/picks\")[0]\n",
    "            for p in data:\n",
    "                buf.append(pick_to_row(p, draft_id))\n",
    "\n",
    "        if buf:\n",
    "            parts.append(pd.DataFrame(buf))\n",
    "            buf = []\n",
    "\n",
    "        if len(urls) > CHUNK_SIZE:\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS_SEC)\n",
    "\n",
    "    picks_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    out_path = os.path.join(DIR_RAW_PICKS, f\"picks_{season}.parquet\")\n",
    "    picks_df.to_parquet(out_path, index=False)\n",
    "    print(f\"[{season_tag}] picks={picks_df.shape}\")\n",
    "    return picks_df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ADP TIME SERIES (month anchored)\n",
    "# -----------------------------\n",
    "def compute_adp_time_series(picks_df: pd.DataFrame, draft_catalog: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = picks_df.copy()\n",
    "    p[\"draft_id\"] = p[\"draft_id\"].astype(str)\n",
    "    p[\"player_id\"] = p[\"player_id\"].astype(str)\n",
    "    p[\"pick_no\"] = pd.to_numeric(p[\"pick_no\"], errors=\"coerce\")\n",
    "\n",
    "    d = draft_catalog.copy()\n",
    "    d[\"draft_id\"] = d[\"draft_id\"].astype(str)\n",
    "\n",
    "    keep_cols = [\n",
    "        \"draft_id\", \"season\", \"start_dt\", \"start_month\",\n",
    "        \"dynasty_class\", \"type\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\",\n",
    "        \"draft_status\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in d.columns]\n",
    "    m = p.merge(d[keep_cols], on=\"draft_id\", how=\"left\")\n",
    "\n",
    "    m = m[m[\"pick_no\"].notna() & m[\"player_id\"].notna()].copy()\n",
    "    m = m[m[\"start_month\"].notna()].copy()\n",
    "    m = m[m[\"dynasty_class\"].isin(list(KEEP_DYNASTY_CLASSES))].copy()\n",
    "\n",
    "    out = (\n",
    "        m.groupby(\n",
    "            [\"season\", \"start_month\", \"player_id\", \"dynasty_class\", \"type\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\"],\n",
    "            dropna=False\n",
    "        )\n",
    "        .agg(\n",
    "            drafts=(\"draft_id\", \"nunique\"),\n",
    "            picks=(\"pick_no\", \"size\"),\n",
    "            adp=(\"pick_no\", \"mean\"),\n",
    "            min_pick=(\"pick_no\", \"min\"),\n",
    "            max_pick=(\"pick_no\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    out[\"adp\"] = out[\"adp\"].round(2)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ✅ NEW: AUCTION PRICE TIME SERIES (month anchored)\n",
    "def compute_auction_price_time_series(picks_df: pd.DataFrame, draft_catalog: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Monthly auction $ time series anchored on draft start_time (via draft_catalog.start_month)\n",
    "    Output is per player_id + format + month aggregates, similar to compute_adp_time_series.\n",
    "    \"\"\"\n",
    "    p = picks_df.copy()\n",
    "    p[\"draft_id\"] = p[\"draft_id\"].astype(str)\n",
    "    p[\"player_id\"] = p[\"player_id\"].astype(str)\n",
    "\n",
    "    if \"md_amount\" not in p.columns:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    p[\"amount\"] = pd.to_numeric(p[\"md_amount\"], errors=\"coerce\")\n",
    "\n",
    "    d = draft_catalog.copy()\n",
    "    d[\"draft_id\"] = d[\"draft_id\"].astype(str)\n",
    "\n",
    "    keep_cols = [\n",
    "        \"draft_id\", \"season\", \"start_dt\", \"start_month\",\n",
    "        \"dynasty_class\", \"type\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\",\n",
    "        \"draft_status\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in d.columns]\n",
    "    m = p.merge(d[keep_cols], on=\"draft_id\", how=\"left\")\n",
    "\n",
    "    # auction only\n",
    "    if \"type\" in m.columns:\n",
    "        m = m[m[\"type\"].astype(str).str.lower() == \"auction\"].copy()\n",
    "\n",
    "    # clean\n",
    "    m = m[m[\"player_id\"].notna()].copy()\n",
    "    m = m[m[\"start_month\"].notna()].copy()\n",
    "    m = m[m[\"amount\"].notna()].copy()\n",
    "\n",
    "    # match ADP TS class restriction\n",
    "    if \"dynasty_class\" in m.columns:\n",
    "        m = m[m[\"dynasty_class\"].isin(list(KEEP_DYNASTY_CLASSES))].copy()\n",
    "\n",
    "    out = (\n",
    "        m.groupby(\n",
    "            [\"season\", \"start_month\", \"player_id\", \"dynasty_class\", \"md_scoring_type\", \"st_teams\", \"st_rounds\", \"is_superflex\"],\n",
    "            dropna=False\n",
    "        )\n",
    "        .agg(\n",
    "            drafts=(\"draft_id\", \"nunique\"),\n",
    "            sales=(\"amount\", \"size\"),\n",
    "            avg_price=(\"amount\", \"mean\"),\n",
    "            med_price=(\"amount\", \"median\"),\n",
    "            min_price=(\"amount\", \"min\"),\n",
    "            max_price=(\"amount\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    for c in [\"avg_price\", \"med_price\", \"min_price\", \"max_price\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").round(2)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN LOOP (per season)\n",
    "# -----------------------------\n",
    "all_adp_parts = []\n",
    "all_draftcat_parts = []\n",
    "\n",
    "# ✅ NEW collectors\n",
    "all_auction_parts = []\n",
    "all_auctioncat_parts = []\n",
    "\n",
    "for season in SEASONS:\n",
    "    season_tag = str(season)\n",
    "\n",
    "    # 1) Discover leagues/users\n",
    "    leagues_df, league_users_df = discover_leagues(season, SEED_USERS)\n",
    "    if leagues_df.empty:\n",
    "        print(f\"[{season_tag}] No leagues discovered. Skipping season.\")\n",
    "        continue\n",
    "\n",
    "    league_ids = leagues_df[\"league_id\"].astype(str).unique().tolist()\n",
    "\n",
    "    # 2) Drafts\n",
    "    drafts_df = fetch_drafts_for_leagues(league_ids, season)\n",
    "    if drafts_df.empty:\n",
    "        print(f\"[{season_tag}] No drafts found. Skipping season.\")\n",
    "        continue\n",
    "\n",
    "    # 3) Draft catalog (safe start_time -> start_dt/start_month)\n",
    "    draft_catalog = build_draft_catalog(drafts_df)\n",
    "\n",
    "    # Save draft catalog snapshot for the season\n",
    "    cat_out_dir = os.path.join(DIR_SNAP_DRAFT_CAT, f\"season={season}\")\n",
    "    os.makedirs(cat_out_dir, exist_ok=True)\n",
    "    draft_catalog.to_parquet(os.path.join(cat_out_dir, \"draft_catalog.parquet\"), index=False)\n",
    "\n",
    "    # ✅ 3b) auction draft catalog snapshot\n",
    "    auction_catalog = build_auction_draft_catalog(draft_catalog)\n",
    "    if not auction_catalog.empty:\n",
    "        auc_cat_out_dir = os.path.join(DIR_SNAP_AUCTION_CAT, f\"season={season}\")\n",
    "        os.makedirs(auc_cat_out_dir, exist_ok=True)\n",
    "        auction_catalog.to_parquet(os.path.join(auc_cat_out_dir, \"auction_draft_catalog.parquet\"), index=False)\n",
    "        print(f\"[{season_tag}] auction_catalog={auction_catalog.shape}\")\n",
    "        all_auctioncat_parts.append(auction_catalog)\n",
    "\n",
    "    # 4) Picks (completed drafts only)  (now includes md_amount when present)\n",
    "    picks_df = fetch_picks_for_completed_drafts(draft_catalog, season)\n",
    "    if picks_df.empty:\n",
    "        print(f\"[{season_tag}] No picks found. Skipping ADP/Auction TS.\")\n",
    "        continue\n",
    "\n",
    "    # 5) ADP time series by month\n",
    "    adp_ts = compute_adp_time_series(picks_df, draft_catalog)\n",
    "\n",
    "    out_dir = os.path.join(DIR_SNAP_ADP_TS, f\"season={season}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    adp_ts.to_parquet(os.path.join(out_dir, \"adp_time_series.parquet\"), index=False)\n",
    "\n",
    "    print(f\"[{season_tag}] adp_ts={adp_ts.shape}\")\n",
    "    print(f\"[OK] wrote season={season} ADP -> {out_dir}\")\n",
    "\n",
    "    all_adp_parts.append(adp_ts)\n",
    "    all_draftcat_parts.append(draft_catalog)\n",
    "\n",
    "    # ✅ 5b) Auction price time series by month\n",
    "    auction_ts = compute_auction_price_time_series(picks_df, draft_catalog)\n",
    "    if not auction_ts.empty:\n",
    "        out_dir_auc = os.path.join(DIR_SNAP_AUCTION_TS, f\"season={season}\")\n",
    "        os.makedirs(out_dir_auc, exist_ok=True)\n",
    "        auction_ts.to_parquet(os.path.join(out_dir_auc, \"auction_price_series.parquet\"), index=False)\n",
    "        print(f\"[{season_tag}] auction_ts={auction_ts.shape}\")\n",
    "        print(f\"[OK] wrote season={season} AUCTION -> {out_dir_auc}\")\n",
    "        all_auction_parts.append(auction_ts)\n",
    "    else:\n",
    "        print(f\"[{season_tag}] auction_ts is empty (no completed auction drafts or no md_amount found).\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# combined outputs across seasons\n",
    "# -----------------------------\n",
    "if all_adp_parts:\n",
    "    adp_all = pd.concat(all_adp_parts, ignore_index=True)\n",
    "    adp_all_path = os.path.join(DIR_SNAP_ADP_TS, \"adp_time_series_ALL.parquet\")\n",
    "    adp_all.to_parquet(adp_all_path, index=False)\n",
    "    print(f\"[OK] wrote combined ADP TS -> {adp_all_path} | shape={adp_all.shape}\")\n",
    "\n",
    "if all_draftcat_parts:\n",
    "    dc_all = pd.concat(all_draftcat_parts, ignore_index=True)\n",
    "    dc_all_path = os.path.join(DIR_SNAP_DRAFT_CAT, \"draft_catalog_ALL.parquet\")\n",
    "    dc_all.to_parquet(dc_all_path, index=False)\n",
    "    print(f\"[OK] wrote combined draft catalog -> {dc_all_path} | shape={dc_all.shape}\")\n",
    "\n",
    "# ✅ NEW: combined auction outputs\n",
    "if all_auction_parts:\n",
    "    auc_all = pd.concat(all_auction_parts, ignore_index=True)\n",
    "    auc_all_path = os.path.join(DIR_SNAP_AUCTION_TS, \"auction_price_series_ALL.parquet\")\n",
    "    auc_all.to_parquet(auc_all_path, index=False)\n",
    "    print(f\"[OK] wrote combined auction TS -> {auc_all_path} | shape={auc_all.shape}\")\n",
    "\n",
    "if all_auctioncat_parts:\n",
    "    ac_all = pd.concat(all_auctioncat_parts, ignore_index=True)\n",
    "    ac_all_path = os.path.join(DIR_SNAP_AUCTION_CAT, \"auction_draft_catalog_ALL.parquet\")\n",
    "    ac_all.to_parquet(ac_all_path, index=False)\n",
    "    print(f\"[OK] wrote combined auction catalog -> {ac_all_path} | shape={ac_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896b287-7d6b-4f92-b19e-de55436aafa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
